---  
title: "Assignment 2"
author: "LAURA YE; laura.ye.lin@gmail.com"  
date: "October 24, 2017"  
output: 
  html_document:  
    toc: yes  

---  

***

# Loading the Data 

Let's load the AOL Search Data Set (extracted from aol_search_data.zip):

```{r, echo=FALSE, comment=NA, warning=FALSE, message=FALSE, cache=TRUE}
library(tidyverse)
library(dplyr)
setwd("C:\\Users\\lye\\Downloads\\aol_search_data")
user_searches <- read_tsv("user_searches.txt")
```

# Cleaning the Data

## Tranforming column names to snake case

See how the current column names look like:

```{r, echo=FALSE, comment=NA, warning=FALSE, message=FALSE}
names(user_searches)
```

Function for transforming a string to snake case

```{r, echo=FALSE, comment=NA, warning=FALSE, message=FALSE}
snake_case <- function (x) {
  s <- gsub("\\.", "_", x) # Replace dots with underscores
  s <- gsub("(.)([A-Z][a-z]+)", "\\1_\\2", s) # Separate with underscores on capitalization
  s <- tolower(gsub("([a-z0-9])([A-Z])", "\\1_\\2", s)) #lowercase
  s <- gsub("__", "_", s) #double to single underscore
  s <- gsub("^[_, .]", "", s) # del first char underscore "_" or period "."
  s <- gsub(' ', '', s) # remove spaces
}
```

Function for converting data frame result column names to snake case
```{r, echo=FALSE, comment=NA, warning=FALSE, message=FALSE}
fix_column_names <- function(x) {
  names(x) <- snake_case(names(x))
  return(x)
}

user_searches <- user_searches %>%
  fix_column_names
```

Check that the column names have been transformed:

```{r, echo=FALSE, comment=NA, warning=FALSE, message=FALSE }
names(user_searches)
```

# Analyzing the Data

## Group the data by "user session"

A "user session" is defined by all events for a given user where there is no more than a thirty minute gap between events. 

First, determine which rows belong to which sessions.

```{r, echo=FALSE, comment=NA, warning=FALSE, message=FALSE }
sessions <- user_searches %>%
  arrange(anon_id, query_time) %>%
  group_by(anon_id) %>%
  mutate(
    time_diff = difftime(query_time, lag(query_time), units="min"),
    new_session = is.na(lag(anon_id)) | (time_diff > 30),
    session_sequence_number = cumsum(as.numeric(new_session)),
    session_id = paste(anon_id, "_", session_sequence_number, sep="")
  ) %>%
  glimpse
```

Now that we have the each query broken assigned to a unique session ID per user, we can summarize its statistics and gather data like number of searches, when the session started, when the session ended, session duration, number of clicks, mean rank of query, and mean number of search terms (separated by " ").

```{r, echo=FALSE, comment=NA, warning=FALSE, message=FALSE}
sessions_stats <- sessions %>%
  group_by(anon_id, session_sequence_number) %>%
  summarise(
    number_searches = n(),
    session_started_at = first(query_time),
    session_ended_at = last(query_time),
    session_length = difftime(session_ended_at, session_started_at, unit="mins"),
    number_clicks = sum(!is.na(click_url)),
    mean_item_rank = mean(item_rank),
    mean_number_search_terms = mean(length(strsplit(query, " ")))
  ) %>%
  glimpse
```

# Plotting the session statistics

## Statistics by session

Distribution of session durations (histogram count)

```{r, echo=FALSE, comment=NA, warning=FALSE, message=FALSE}
require(ggplot2)

ggplot(sessions_stats, aes(session_length)) + 
  geom_histogram(binwidth = 5) + 
  geom_density(aes(y=5 * ..count.., colour="red"), size=1,show.legend=FALSE) + 
  ggtitle("AOL Sessions - Distribution of Session Duration") +
  xlim(c(0, 80)) + ylim(c(0, 75000)) +
  xlab("Session Duration") +
  ylab("Count")
```

Distribution of the number of clicks per session (histogram count)

```{r, echo=FALSE, comment=NA, warning=FALSE, message=FALSE}
ggplot(sessions_stats, aes(number_clicks)) + 
  geom_histogram(binwidth = 1) + 
  geom_density(aes(y=1 * ..count.., colour="red"), size=1,show.legend=FALSE) + 
  ggtitle("AOL Sessions - Distribution of Clicks per Session") +
  xlim(c(0, 40)) + ylim(c(0, 75000)) +
  xlab("Number of Clicks") +
  ylab("Count")
```

## Statistics by user

First, calculate the statistics by user

```{r, echo=FALSE, comment=NA, warning=FALSE, message=FALSE}
user_stats <- sessions_stats %>%
  group_by(anon_id) %>%
  summarise(
    sessions_count = n(),
    mean_session_length = mean(session_length)
  ) %>%
  glimpse
```

Distribution of the number of sessions by user (histogram count)

```{r, echo=FALSE, comment=NA, warning=FALSE, message=FALSE}
ggplot(user_stats, aes(sessions_count)) + 
  geom_histogram(binwidth = 1) + 
  geom_density(aes(y=1 * ..count.., colour="red"), size=1,show.legend=FALSE) + 
  ggtitle("AOL Visitors - Distribution of Sessions per User") +
  xlim(c(0, 40)) + ylim(c(0, 10000)) +
  xlab("Number of Sessions") +
  ylab("Number of Users")
```

Distribution of the mean session duration by user (histogram count)

```{r, echo=FALSE, comment=NA, warning=FALSE, message=FALSE}
ggplot(user_stats, aes(mean_session_length)) + 
  geom_histogram(binwidth = 1) + 
  geom_density(aes(y=1 * ..count.., colour="red"), size=1,show.legend=FALSE) + 
  ggtitle("AOL Visitors - Distribution of Mean Session Length per User") +
  xlim(c(0, 30)) + ylim(c(0, 10000)) +
  xlab("Mean Session Duration") +
  ylab("Number of Users")
```

## Statistics by query_time (hour)

We will convert the query_time to its hour, then add this value as a column to the ```user_searches``` data frame,

```{r, echo=FALSE, comment=NA, warning=FALSE, message=FALSE}

user_searches_time <- user_searches %>%
  mutate(query_time_hour = lubridate::hour(strptime(query_time, format="%Y-%m-%d %H:%M:%S")))

```

There is now a new column that we can use to group the queries by the hour and determine stats for the queries by the hour.

```{r, echo=FALSE, comment=NA, warning=FALSE, message=FALSE}
time_of_day_stats <- user_searches_time %>%
  group_by(query_time_hour) %>%
  summarise(
    query_count = n(),
    number_clicks = sum(!is.na(click_url))
  ) %>% 
  glimpse
```

Number of queries during each hour

```{r, echo=FALSE, comment=NA, warning=FALSE, message=FALSE}
ggplot(user_searches_time, aes(query_time_hour)) + 
  geom_bar() + 
  ggtitle("AOL Visitors - Number of Queries by Hour of Day") +
  xlab("Hour of Day") +
  ylab("Number of Queries")
```

Number of clicks during each hour

```{r, echo=FALSE, comment=NA, warning=FALSE, message=FALSE}

ggplot(user_searches_time %>% na.omit(), aes(query_time_hour)) + 
  geom_bar() + 
  ggtitle("AOL Visitors - Number of Queries by Hour of Day") +
  xlab("Hour of Day") +
  ylab("Number of Clicks")
```

For both queries and clicks, the minimum would occur around 4:00-5:00 and the maximum occurred around 21:00.