---
title: "Tree-Based Models - Data Camp"
output: html_notebook
author: "Laura Ye"
---

# Welcome

Supervised learning - subfield of machine learning
Unsupervised learning - learn from input data alone

Input - Output/Label/Number

Supervised learning can map new features to label/number

Decision-tree based models
1. Random forest
2. Gradient boosting machines

Differences
- Unique combo of modeling interpretability
- Ease of use
- Excellent accuracy
- Make decisions + numeric predictions

Types of Trees
- Classification & Regression Trees
- Bagged Trees
- Random Forests
- Boosted Trees (GBM)

Decision tree Terminology

Nodes
Root node at the top
Leaf nodes end
Internal nodes are neither leaf or root

Training Decision Trees in R
- Use rpart package, stands for "recursive partitioning"

rpart(response ~ ., data = dataset)

## Exercises

Build a classification tree

Let's get started and build our first classification tree. A classification tree is a decision tree that performs a classification (vs regression) task.

You will train a decision tree model to understand which loan applications are at higher risk of default using a subset of the German Credit Dataset. The response variable, called "default", indicates whether the loan went into a default or not, which means this is a binary classification problem (there are just two classes).

```{r}
# Look at the data
str(creditsub)

# Create the model
credit_model <- rpart(formula = default ~ ., 
                      data = creditsub, 
                      method = "class")

# Display the results
rpart.plot(x = credit_model, yesno = 2, type = 0, extra = 0)
```

# Intro to Classification Trees

Advantages
- Simple to understand, interpret, visualize - just need to understand a flow chart
- Can handle both numerical and categorical inputs natively (don't need to make dummy variables)
  - root splitting is used in some models to handle cateogrical inputs
- can handle missing data elegantly
  - popular method: choose left or right at split at random, or go both ways and average the leaves for final result
- robust to outliers
- requires little data prep
- can model non-linearity
- can be trained quickly on big data sets

Disadvantage
- Large trees are hard to interpret
- Trees have high variance, so poor model performance
- easily overfitted

# Overview of modeling process

Train/Test Split - usually 80/20
Use cross-validation

Get number of rows in data set (nrow)
Define how many rows for training set
Set seed
Create a vector of indices which is 80% of random sample (sample)
Subset the data

Train a Classification  Tree
rpart(formula, data, method)
method = "class" for classification tree (binary response)

## Exercises

Train/test split

For this exercise, you'll randomly split the German Credit Dataset into two pieces: a training set (80%) called credit_train and a test set (20%) that we will call credit_test. We'll use these two sets throughout the chapter.

```{r}
# Total number of rows in the credit data frame
n <- nrow(credit)

# Number of rows for the training set (80% of the dataset)
n_train <- round(0.8 * n) 

# Create a vector of indices which is an 80% random sample
set.seed(123)
train_indices <- sample(1:n, n_train)

# Subset the credit data frame to training indices only
credit_train <- credit[train_indices, ]  
  
# Exclude the training indices to create the test set
credit_test <- credit[-train_indices, ]  
```

Train a classification tree model

In this exercise, you will train a model on the newly created training set and print the model object.

```{r}
# Train the model (to predict 'default')
credit_model <- rpart(formula = default ~ ., 
                      data = credit_train, 
                      method = "class")

# Look at the model output                      
print(credit_model)
```

# Evaluating classification model performance

Make a prediction and evaluate predictions

Predict function
predict(model, test_dataset)
rpart predict has type argument - if type="class", returns classification labels, if type="prob", raw predicted values will be returned

Evaluation metrics for binary classification
Accuracy
Confusion matrix
Log-loss
AUC

Accuracy
Measures how often the classified predicts the class correctly
Makes no distinction between classes
Might want to look at specific classes

Confusion Matrix
Detailed breakdown of correct/incorrect predictions
In binary classification, 2x2
Good model will have larger numbers in the main diagonal
Can be expanded for more classes

Use confusionMatrix(data, reference) in caret package to create a confusion matrix

## Exercise

Compute confusion matrix

As discussed in the previous video, there are a number of different metrics by which you can measure the performance of a classification model. In this exercise, we will evaluate the performance of the model using test set classification error. A confusion matrix is a convenient way to examine the per-class error rates for all classes at once.

```{r}
# Generate predicted classes using the model object
class_prediction <- predict(object = credit_model,  
                        newdata = credit_test,   
                        type = "class")  
                            
# Calculate the confusion matrix for the test set
confusionMatrix(data = class_prediction,       
                reference = credit_test$default)  
```

# Splitting criterion in trees

Classification trees using split criterion to split labels
At each node, the tree decides whether to go left or right
Split into subsets where each subset only belongs to one class
Real data: completely pure data may not be possible
Decision boundaries separate the regions where each subset is 100% pure

How to determine best split?
Most homogenous subset is the best split & preferred partition
Need a way to measure purity of splits
Works better mathematically to measure impurity vs measuring purity

Impurity Measure - Gini Index
Higher = less pure
Lower = more pure
Decision tree will select the split with the lower gini index

## Exercise

Compare models with a different splitting criterion

Train two models that use a different splitting criterion and use the validation set to choose a "best" model from this group. To do this you'll use the parms argument of the rpart() function. This argument takes a named list that contains values of different parameters you can use to change how the model is trained. Set the parameter split to control the splitting criterion.

```{r}
# Train a gini-based model
credit_model1 <- rpart(formula = default ~ ., 
                       data = credit_train, 
                       method = "class",
                       parms = list(split = "gini"))

# Train an information-based model
credit_model2 <- rpart(formula = default ~ ., 
                       data = credit_train, 
                       method = "class",
                       parms = list(split = "information"))

# Generate predictions on the validation set using the gini model
pred1 <- predict(object = credit_model1, 
             newdata = credit_test,
             type = "class")    

# Generate predictions on the validation set using the information model
pred2 <- predict(object = credit_model2, 
             newdata = credit_test,
             type = "class")

# Compare classification error
ce(actual = credit_test$default, 
   predicted = pred1)
ce(actual = credit_test$default, 
   predicted = pred2)  
```

CE stands for classification error, which is the fraction of incorrectly classified instances. Lower CE indicates that the credit model using the information index had fewer errors in classification.

# Intro to regression trees

Regression model - predict a numeric outcome from a set of inputs
Inputs can be numeric or categorical
For example, predicting weight.

How to determine accuracy of regression model? +/- threshold?

Root node - data is partitioned by the feature that will increase the homogeneity.
Homogeneity is measured by variance, stdev or abs dev from mean.

rpart(formula, data, method = "anova", control)
method = "anova" for regression tree
method = "class" for classification tree

Split the data differently into train, validation, test sets
Validation - new concept, like test set, used to measure performance on unseen data. Used to tune the paramters
Test set - use only once at the end of the modeling process

## Exercise

Split the data

These examples will use a subset of the Student Performance Dataset from UCI ML Dataset Repository.

The goal of this exercise is to predict a student's final Mathematics grade based on the following variables: sex, age, address, studytime (weekly study time), schoolsup (extra educational support), famsup (family educational support), paid (extra paid classes within the course subject) and absences.

After the initial exploration, let's split the data into training, validation, test sets. In this chapter, we will introduce the idea of a validation set, which can be used to select a "best" model from a set of competing models.

```{r}
# Look/explore the data
str(grade)

# Randomly assign rows to ids (1/2/3 represents train/valid/test)
# This will generate a vector of ids of length equal to the number of rows
# The train/valid/test split will be approximately 70% / 15% / 15% 
set.seed(1)
assignment <- sample(1:3, size = nrow(grade), prob = c(0.7,0.15,0.15), replace = TRUE)

# Create a train, validation and tests from the original data frame 
grade_train <- grade[assignment == 1, ]    # subset the grade data frame to training indices only
grade_valid <- grade[assignment == 2, ]  # subset the grade data frame to validation indices only
grade_test <- grade[assignment == 3, ]   # subset the grade data frame to test indices only
```

Train a regression tree model

In this exercise, we will use the grade_train dataset to fit a regression tree using rpart() and visualize it using rpart.plot(). A regression tree plot will look identical to a classification tree plot, with the exception that there will be numeric values in the leaf nodes instead of predicted classes.

```{r}
# Train the model
grade_model <- rpart(formula = final_grade ~ ., 
                     data = grade_train, 
                     method = "anova")

# Look at the model output                      
print(grade_model)

# Plot the tree model
rpart.plot(x = grade_model, yesno = 2, type = 0, extra = 0)
```

# Performance metrics for regression

Evaluate the performance of the model
With classification - accuracy is okay
Evaluate using a different metric
Resposne is real-valued - measure how far away the predictions are from actual values
MAE - mean absolute error
RMSE - root mean square error
Lower is better

RMSE punishes large errors harsher than MAE
RMSE is more useful when large errors are more undesirable

Appropriate metric is dependent on use case
library(Metrics)
rmse(actual, predicted)

## Exercises

Evaluate a regression tree model

Predict the final grade for all students in the test set. The grade is on a 0-20 scale. Evaluate the model based on test set RMSE (Root Mean Squared Error). RMSE tells us approximately how far away our predictions are from the true values.

```{r}
# Generate predictions on a test set
pred <- predict(object = grade_model,   # model object 
                newdata = grade_test)  # test dataset

# Compute the RMSE
rmse(actual = grade_test$final_grade, 
     predicted = pred)
```

# What are the hyperparameters of a decision tree?

Room for improvement
Knobs to turn to affect tree growth - hyperparameters

rpart function has control parameter
if control parameter is not used then default values will be used

parameters:
size and complexity of tree: minsplit (minimum number of data points required to attempt a split, default=20), cp (complexity parameter, default = 0.1), maxdepth (depth of a decision tree, default = 30)

Cost-complexity parameter (CP)
rpart stores cP in a table in the model or use plotcp(model)
find the row and CP value with least xerror
prune(tree = model, cp = cp_opt) - function to prune the tree and make it better

## Exercises

Tuning the model

Tune (or "trim") the model using the prune() function by finding the best "CP" value (CP stands for "Complexity Parameter").

```{r}
# Plot the "CP Table"
plotcp(grade_model)

# Print the "CP Table"
print(grade_model$cptable)

# Retreive optimal cp value based on cross-validated error
opt_index <- which.min(grade_model$cptable[, "xerror"])
cp_opt <- grade_model$cptable[opt_index, "CP"]

# Prune the model (to optimized cp value)
grade_model_opt <- prune(tree = grade_model, 
                         cp = cp_opt)
                          
# Plot the optimized model
rpart.plot(x = grade_model_opt, yesno = 2, type = 0, extra = 0)
```

# Grid search for model selection

Model selection or hyperparameter selection
Grid search - model technique
Construct a grid and train a sequence of models and evaluate them

Important definitions:
Grid search - exhaustive search through specified hyperparameters
Grid - hyperparameter combos that will be iterated over
Goals - iterate a large number of parameter settings to find the best model
How is it chosen? Choose approrpriate performance metric
Performance of each model is evaluated - best one is selected as a winner
Run small grid, see if minimum and maximum is within the range, then expand

Establish a list of possible values for hyperparameters
expand.grid(column names) can combine these hyperparameters into a grid
Create empty lists to store the models
Use a loop in R to iterate the rows of the grid
Evaluate models using rmse
Find the model with the min rmse

## Exercises

Generate a grid of hyperparameter values

Use expand.grid() to generate a grid of maxdepth and minsplit values.

```{r}
# Establish a list of possible values for minsplit and maxdepth
minsplit <- seq(1, 4, 1)
maxdepth <- seq(1, 6, 1)

# Create a data frame containing all combinations 
hyper_grid <- expand.grid(minsplit = minsplit, maxdepth = maxdepth)

# Check out the grid
head(hyper_grid)

# Print the number of grid combinations
nrow(hyper_grid)
```

Generate a grid of models

In this exercise, we will write a simple loop to train a "grid" of models and store the models in a list called grade_models. R users who are familiar with the apply functions in R could think about how this loop could be easily converted into a function applied to a list as an extra-credit thought experiment.

```{r}
# Number of potential models in the grid
num_models <- nrow(hyper_grid)

# Create an empty list to store models
grade_models <- list()

# Write a loop over the rows of hyper_grid to train the grid of models
for (i in 1:num_models) {

    # Get minsplit, maxdepth values at row i
    minsplit <- hyper_grid$minsplit[i]
    maxdepth <- hyper_grid$maxdepth[i]

    # Train a model and store in the list
    grade_models[[i]] <- rpart(formula = final_grade ~ ., 
                               data = grade_train, 
                               method = "anova",
                               minsplit = minsplit,
                               maxdepth = maxdepth)
}
```

Evaluate the grid

Earlier in the chapter we split the dataset into three parts: training, validation and test.

```{r}
# Number of potential models in the grid
num_models <- length(grade_models)

# Create an empty vector to store RMSE values
rmse_values <- c()

# Write a loop over the models to compute validation RMSE
for (i in 1:num_models) {

    # Retreive the i^th model from the list
    model <- grade_models[[i]]
    
    # Generate predictions on grade_valid 
    pred <- predict(object = model,
                    newdata = grade_valid)
    
    # Compute validation RMSE and add to the 
    rmse_values[i] <- rmse(actual = grade_valid$final_grade, 
                           predicted = pred)
}

# Identify the model with smallest validation set RMSE
best_model <- grade_models[[which.min(rmse_values)]]

# Print the model paramters of the best model
best_model$control

# Compute test set RMSE on best_model
pred <- predict(object = best_model,
                newdata = grade_test)
rmse(actual = grade_test$final_grade, 
     predicted = grade_test$final_grade)
```

# Intro to bagged trees

Major drawback to decision trees is high variance
- Small change can result in different split

Bagged trees averages many trees into one
- Easiest way to create an ensemble
- Can be used with other methods besides trees
- Prevent overfitting

Bagging is shorthand for bootstrap aggregating
- sample rows at random with replacement
- possible for single training example to be drawn more than once
- some rows are represented multiple times and some are absent
- similar data than what we started with

1. draw samples with replacement from training set (usually 0.5n)
2. train a model for each bootstrapped sample. The more the better
3. Each bootstrapped is considered in the average to get a final prediction.
4. Bagged prediction "The wisdom of the crowds"

library(ipred)
bagging(formula, data)

## Exercises

Train a bagged tree model

Let's start by training a bagged tree model. You'll be using the bagging() function from the ipredpackage. The number of bagged trees can be specified using the nbagg parameter, but here we will use the default (25).

If we want to estimate the model's accuracy using the "out-of-bag" (OOB) samples, we can set the the coob parameter to TRUE. The OOB samples are the training obsevations that were not selected into the bootstrapped sample (used in training). Since these observations were not used in training, we can use them instead to evaluate the accuracy of the model (done automatically inside the bagging() function).

```{r}
# Bagging is a randomized model, so let's set a seed (123) for reproducibility
set.seed(123)

# Train a bagged model
credit_model <- bagging(formula = default ~ ., 
                        data = credit_train,
                        coob = TRUE)

# Print the model
print(credit_model)
```

# Evaluating bagged tree performance

class_predictions <- predict(model_object, newdata, type)
type = "class" will return classification labels

confusionMatrix(data, reference)
ROC Curve - useful tool for classification
AUC curve - area under the curve (larger is better)

library(Metrics)
auc(actual, predicted)
- function returns a single value

## Exercises

Prediction and confusion matrix

As you saw in the video, a confusion matrix is a very useful tool for examining all possible outcomes of your predictions (true positive, true negative, false positive, false negative).

In this exercise, you will predict those who will default using bagged trees. You will also create the confusion matrix using the confusionMatrix() function from the caret package.

```{r}
# Generate predicted classes using the model object
class_prediction <- predict(object = credit_model,    
                            newdata = credit_test,  
                            type = "class")  # return classification labels

# Print the predicted classes
print(class_prediction)

# Calculate the confusion matrix for the test set
confusionMatrix(data = class_prediction,       
                reference = credit_test$default)  
```

Predict on a test set and compute AUC

In binary classification problems, we can predict numeric values instead of class labels. In fact, class labels are created only after you use the model to predict a raw, numeric, predicted value for a test point.

The predicted label is generated by applying a threshold to the predicted value, such that all tests points with predicted value greater than that threshold get a predicted label of "1" and, points below that threshold get a predicted label of "0".

In this exercise, generate predicted values (rather than class labels) on the test set and evaluate performance based on AUC (Area Under the ROC Curve). The AUC is a common metric for evaluating the discriminatory ability of a binary classification model.

```{r}
# Generate predictions on the test set
pred <- predict(object = credit_model,
                newdata = credit_test,
                type = "prob")

# `pred` is a matrix
class(pred)
                
# Look at the pred format
head(pred)
                
# Compute the AUC (`actual` must be a binary (or 1/0 numeric) vector)
auc(actual = ifelse(credit_test$default == "yes", 1, 0), 
    predicted = pred[,"yes"])                    
```

# Use caret for cross-validating models

Single test and train set can affect the model if they have big variance depending on how you split the data.
CV - average multiple estimates

Most popular: K-fold cross-validation
Partition into K subsets of equal sizes
Run K-1 separate experiments - one of them will be a test set
Evaluate model's performance on the test set
K-1 estimates of test set AUC and average of that is the CV estimate of AUC
CV takes longer but more accurate, so recommended for smaller dataset

caret package
```{r}
library(caret)
set.seed(1)
ctrl <- trainControl(method = "cv", # cross-validation
              number = 5,  # 5 folds
              classProbs=TRUE, # For AUC
              summaryFunction = twoClassSummary) #For AUC

train(formula,
      data,
      method = "treebag",
      metric = "ROC",
      trControl = ctrl)
```

## Exercises

Cross-validate a bagged tree model in caret

Use caret::train() with the "treebag" method to train a model and evaluate the model using cross-validated AUC. The caret package allows the user to easily cross-validate any model across any relevant performance metric. In this case, we will use 5-fold cross validation and evaluate cross-validated AUC (Area Under the ROC Curve).

```{r}
# Specify the training configuration
ctrl <- trainControl(method = "cv",     # Cross-validation
                     number = 5,      # 5 folds
                     classProbs = TRUE,                  # For AUC
                     summaryFunction = twoClassSummary)  # For AUC

# Cross validate the credit model using "treebag" method; 
# Track AUC (Area under the ROC curve)
set.seed(1)  # for reproducibility
credit_caret_model <- train(default ~ .,
                            data = credit_train, 
                            method = "treebag",
                            metric = "ROC",
                            trControl = ctrl)

# Look at the model object
print(credit_caret_model)

# Inspect the contents of the model list 
names(credit_caret_model)

# Print the CV AUC
credit_caret_model$results[,"ROC"]
```

Generate predictions from the caret model

Generate predictions on a test set for the caret model.

```{r}
# Generate predictions on the test set
pred <- predict(object = credit_caret_model, 
                newdata = credit_test,
                type = "prob")

# Compute the AUC (`actual` must be a binary (or 1/0 numeric) vector)
auc(actual = ifelse(credit_test$default == "yes", 1, 0), 
                    predicted = pred[,"yes"])
```

Compare test set performance to CV performance

In this excercise, you will print test set AUC estimates that you computed in previous exercises. These two methods use the same code underneath, so the estimates should be very similar.

```{r}
# Print ipred::bagging test set AUC estimate
print(credit_ipred_model_test_auc)

# Print caret "treebag" test set AUC estimate
print(credit_caret_model_test_auc)
                
# Compare to caret 5-fold cross-validated AUC
credit_caret_model$results[, "ROC"]
```

# Intro to Random Forest

Improvement on bagged trees
Better performance
Simpler to train and tune
Similar to boosted trees

RF: Sample of trees based on bootstrapped samples
- Difference: randomness is added at each split - only consider a random sample of subset of features instead of ALL of the features
- leads to trees that are more different and improves upon bagging by reducing correlated trees

Packages: randomForest (oldest), ranger, etc

> randomForest package has varImp() function, which when passed a rF model, which outputs a list of important features. Best to use to help interpret randomForest model.

```{r}
library(randomForest)
randomForest(formula, data)
```

Default RF has 500 trees

## Exercises

Train a Random Forest model

Here you will use the randomForest() function from the randomForest package to train a Random Forest classifier to predict loan default.

```{r}
# Train a Random Forest
set.seed(1)  # for reproducibility
credit_model <- randomForest(formula = default ~ ., 
                             data = credit_train)
                             
# Print the model output                             
print(credit_model)
```

> Be careful about missing data and using character vs factored data. Factors are preferred. Don't use factor features with only one level!

# Understanding RF model output

What do the outputs mean?
Type of random forest: Either classification or regression
Number of trees: 500 by default
Number of variables tried at each split: mtry
OOB estimate of error: out of bag estimate, error across samples not selected into boostrap training set
confusion matrix: because classification

bootstrap samples
absent samples = out of bag samples
built-in validation set without extra work
classification error across all teh samples is oob error
rows represent number of trees in the forest
i'th row is the error up to and including that tree

last row shows final oob error
plot of RF model object is trees vs error - to figure out how many are necessary
error rates level out around 300 trees - reduce computing power

## Exercises

Evaluate out-of-bag error

Here you will plot the OOB error as a function of the number of trees trained, and extract the final OOB error of the Random Forest model from the trained model object.

```{r}
# Grab OOB error matrix & take a look
err <- credit_model$err.rate
head(err)

# Look at final OOB error rate (last row in err matrix)
oob_err <- err[nrow(err), "OOB"]
print(oob_err)

# Plot the model trained in the previous exercise
plot(credit_model)

# Add a legend since it doesn't have one by default
legend(x = "right", 
       legend = colnames(err),
       fill = 1:ncol(err))
```

Evaluate model performance on a test set

Use the caret::confusionMatrix() function to compute test set accuracy and generate a confusion matrix. Compare the test set accuracy to the OOB accuracy.

```{r}
# Generate predicted classes using the model object
class_prediction <- predict(object = credit_model,   # model object 
                            newdata = credit_test,  # test dataset
                            type = "class") # return classification labels
                            
# Calculate the confusion matrix for the test set
cm <- confusionMatrix(data = class_prediction,       # predicted classes
                      reference = credit_test$default)  # actual classes
print(cm)

# Compare test set accuracy to OOB accuracy
paste0("Test Accuracy: ", cm$overall[1])
paste0("OOB Accuracy: ", 1 - oob_err)
```

# OOB Error vs test set error

Advantages of OOB estimates for RF
- Can evaluate model without a separate test set
- Computed automatically - no need for extra code

Disadvantages
- Only estimates error, no AUC or log-loss. No way to compute them after the fact
- Cannot compare RF performance to other types of models

## Exercises

Evaluate test set AUC

In Chapter 3, we learned about the AUC metric for evaluating binary classification models. In this exercise, you will compute test set AUC for the Random Forest model.

```{r}
# Generate predictions on the test set
pred <- predict(object = credit_model,
            newdata = credit_test,
            type = "prob")

# `pred` is a matrix
class(pred)
                
# Look at the pred format
head(pred)
                
# Compute the AUC (`actual` must be a binary 1/0 numeric vector)
auc(actual = ifelse(credit_test$default == "yes", 1, 0), 
    predicted = pred[,"yes"])                    
```

# Tuning a Random Forest Model

Random forest is one of the easiest to tune
Handful of parameters with large impact
Great beginner model

Hyperparameters
ntree - number of trees in the forest
mtry - controls variability and randomness
sampsize - controls variability and randomness 63.2% - expected number of unique observations in a bootstrapped sample
nodesize - control complexity. small = deeper complex trees
maxnodes - controls complexity. avoids overfitting

mtry: tune with tuneRF()
number of predictor variables that we sample at each split
tuneRF(x, y, ntreeTry)
Starts with default of mtry, and increase value at a step size (default = 2)
Stops when the error stops changing

Grid search is helpful here

## Exercises

Tuning a Random Forest via mtry

In this exercise, you will use the randomForest::tuneRF() to tune mtry (by training several models). This function is a specific utility to tune the mtry parameter based on OOB error, which is helpful when you want a quick & easy way to tune your model. A more generic way of any Random Forest parameter will be presented in the following exercise.

```{r}
# Execute the tuning process
set.seed(1)              
res <- tuneRF(x = subset(credit_train, select = -default),
              y = credit_train$default,
              ntreeTry = 500)
               
# Look at results
print(res)

# Find the mtry value that minimizes OOB Error
mtry_opt <- res[,"mtry"][which.min(res[,"OOBError"])]
print(mtry_opt)

# If you just want to return the best RF model (rather than results)
# you can set `doBest = TRUE` in `tuneRF()` to return the best RF model
# instead of a set performance matrix.
```

Tuning a Random Forest via tree depth

In Chapter 2, we created a manual grid of hyperparameters using the expand.grid() function and wrote code that trained and evaluated the models of the grid in a loop. In this exercise, you will create a grid of mtry, nodesize and sampsize values. In this example, we will identify the "best model" based on OOB error. The best model is defined as the model from our grid which minimizes OOB error.

```{r}
# Establish a list of possible values for mtry, nodesize and sampsize
mtry <- seq(4, ncol(credit_train) * 0.8, 2)
nodesize <- seq(3, 8, 2)
sampsize <- nrow(credit_train) * c(0.7, 0.8)

# Create a data frame containing all combinations 
hyper_grid <- expand.grid(mtry = mtry, nodesize = nodesize, sampsize = sampsize)

# Create an empty vector to store OOB error values
oob_err <- c()

# Write a loop over the rows of hyper_grid to train the grid of models
for (i in 1:nrow(hyper_grid)) {

    # Train a Random Forest model
    model <- randomForest(formula = default ~ ., 
                          data = credit_train,
                          mtry = hyper_grid$mtry[i],
                          nodesize = hyper_grid$nodesize[i],
                          sampsize = hyper_grid$sampsize[i])
                          
    # Store OOB error for the model                      
    oob_err[i] <- model$err.rate[nrow(model$err.rate), "OOB"]
}

# Identify optimal set of hyperparmeters based on OOB error
opt_i <- which.min(oob_err)
print(hyper_grid[opt_i,])
```

# Intro to boosting

Decision trees, bagged trees, random forests

Boosted trees - another tree based
Gradient boosting machine using gbm boost

Boosting Algorithms: Adaboost, Gradient Boosting Machine (GBM)

Adaboost Algorithm
- Train a decision tree where each decision has equal weight
- Lower weights if decision was easy to classify
- Second tree is grown on weighted data
- New model =  Tree 1 + tree 2
- Classification error from this new 2-tree ensemble
- Grow 3rd tree to predict the revised residuals
- Repeat for N iterations (specified). Classifies observations that were not easy for preceding tree
- Final model is a weighted sum

GBM
- Gradient descent + boosting
- Introduces weak learner (decision tree) to compensate for the shortcomings
- Adaboost: shortcomings have high-weight data points
- GB: shortcomings are identified by gradients

Advantages
- Performs better than any other algorithm
- Optimizes a user-defined cost function. Others minimize a loss function that you cannot define

Disadvantages
- Tends to overfit (set an early stopping point)
- Sensitive to extreme values and noise

Train GBM: gbm(formula, distribution, data, n.trees)
distribution = bernoulli for binary classification
n.trees = default 100 - reasonable for quick estimate

## Exercises

Train a GBM model

Here you will use the gbm() function to train a GBM classifier to predict loan default. You will train a 10,000-tree GBM on the credit_train dataset, which is pre-loaded into your workspace.

Using such a large number of trees (10,000) is probably not optimal for a GBM model, but we will build more trees than we need and then select the optimal number of trees based on early performance-based stopping. The best GBM model will likely contain fewer trees than we started with.

For binary classification, gbm() requires the response to be encoded as 0/1 (numeric), so we will have to convert from a "no/yes" factor to a 0/1 numeric response column.

Also, the the gbm() function requires the user to specify a distribution argument. For a binary classification problem, you should set distribution = "bernoulli". The Bernoulli distribution models a binary response.

```{r}
# Convert "yes" to 1, "no" to 0
credit_train$default <- ifelse(credit_train$default == "yes", 1, 0)

# Train a 10000-tree GBM model
set.seed(1)
credit_model <- gbm(formula = default ~ ., 
                    distribution = "bernoulli", 
                    data = credit_train,
                    n.trees = 10000)
                    
# Print the model object                    
print(credit_model)

# summary() prints variable importance
summary(credit_model)
```

# Understanding GBM Model Output

Print(GBM_Model)
- n.trees = number of iterations
- Also if there are any predictors with non-zero or zero influence. Noise variables or no correlation variables would be picked up.

Summary(GBM_Model) and Summary(RF_Model)
- variable importance table

predict(model, type = "response", n.trees = 10000)
- predict is an alias for a gbm specific version
- fullname is predict.gbm
- need to specify number of trees. no default value
- type specified controls the type of output

## Exercises

Prediction using a GBM model

The gbm package uses a predict() function to generate predictions from a model, similar to many other machine learning packages in R. When you see a function like predict() that works on many different types of input (a GBM model, a RF model, a GLM model, etc), that indicates that predict() is an "alias" for a GBM-specific version of that function. The GBM specific version of that function is predict.gbm(), but for convenience sake, we can just use predict() (either works).

One thing that's particular to the predict.gbm() however, is that you need to specify the number of trees used in the prediction. There is no default, so you have to specify this manually. For now, we can use the same number of trees that we specified when training the model, which is 10,000 (though this may not be the optimal number to use).

Another argument that you can specify is type, which is only relevant to Bernoulli and Poisson distributed outcomes. When using Bernoulli loss, the returned value is on the log odds scale by default and for Poisson, it's on the log scale. If instead you specify type = "response", then gbm converts the predicted values back to the same scale as the outcome. This will convert the predicted values into probabilities for Bernoulli and expected counts for Poisson.

```{r}
# Since we converted the training response col, let's also convert the test response col
credit_test$default <- ifelse(credit_test$default == "yes", 1, 0)

# Generate predictions on the test set
preds1 <- predict(object = credit_model, 
                  newdata = credit_test,
                  n.trees = 10000)

# Generate predictions on the test set (scale to response)
preds2 <- predict(object = credit_model, 
                  newdata = credit_test,
                  n.trees = 10000,
                  type = "response")

# Compare the range of the two sets of predictions
range(preds1)
range(preds2)
```

Evaluate test set AUC

Compute test set AUC of the GBM model for the two sets of predictions. We will notice that they are the same value. That's because AUC is a rank-based metric, so changing the actual values does not change the value of the AUC.

However, if we were to use a scale-aware metric like RMSE to evaluate performance, we would want to make sure we converted the predictions back to the original scale of the response.

```{r}
# Generate the test set AUCs using the two sets of preditions & compare
auc(actual = credit_test$default, predicted = preds1)  #default
auc(actual = credit_test$default, predicted = preds2)  #rescaled
```

# GBM Hyperparameters - Tuning a GBM model

Tuning = early stopping

Hyperparameters
n.trees: # of trees
bag.fraction: proportion of observations to be sampled in each tree
n.minobsinnode: min number of observations in the trees terminal nodes
interaction.depth: maximum nodes per tree (num of nodes per tree)
shrinkage: learning rate

better to take many small steps - easier to correct than a big one
- slowing learning rate will make the training process longer

early stopping - stopping in iterative system based on performance in a holdout set
- can be oob sample
- separate validation set 
- cross-validation

after the validation error decreases and stablizes, before it starts to increase because of overfitting

gbm(formula, distribution, data, n.trees, cv.folds)
gbm.perf(model, method="cv")
- returns optimal ntree based on CV error
gbm.perf(model, method="oob")
- returns optimal ntree based on oob error

## Exercises

Early stopping in GBMs

Use the gbm.perf() function to estimate the optimal number of boosting iterations (aka n.trees) for a GBM model object using both OOB and CV error. When you set out to train a large number of trees in a GBM (such as 10,000) and you use a validation method to determine an earlier (smaller) number of trees, then that's called "early stopping". The term "early stopping" is not unique to GBMs, but can describe auto-tuning the number of iterations in an iterative learning algorithm.

```{r}
# Optimal ntree estimate based on OOB
ntree_opt_oob <- gbm.perf(object = credit_model, 
                          method = "OOB", 
                          oobag.curve = TRUE)

# Train a CV GBM model
set.seed(1)
credit_model_cv <- gbm(formula = default ~ ., 
                       distribution = "bernoulli", 
                       data = credit_train,
                       n.trees = 10000,
                       cv.folds = 2)

# Optimal ntree estimate based on CV
ntree_opt_cv <- gbm.perf(object = credit_model_cv, 
                         method = "cv")
 
# Compare the estimates                         
print(paste0("Optimal n.trees (OOB Estimate): ", ntree_opt_oob))                         
print(paste0("Optimal n.trees (CV Estimate): ", ntree_opt_cv))
```

OOB vs CV-based early stopping

In the previous exercise, we used OOB error and cross-validated error to estimate the optimal number of trees in the GBM. These are two different ways to estimate the optimal number of trees, so in this exercise we will compare the performance of the models on a test set. We can use the same model object to make both of these estimates since the predict.gbm() function allows you to use any subset of the total number of trees (in our case, the total number is 10,000).

```{r}
# Generate predictions on the test set using ntree_opt_oob number of trees
preds1 <- predict(object = credit_model, 
                  newdata = credit_test,
                  n.trees = ntree_opt_oob)
                  
# Generate predictions on the test set using ntree_opt_cv number of trees
preds2 <- predict(object = credit_model, 
                  newdata = credit_test,
                  n.trees = ntree_opt_cv)   

# Generate the test set AUCs using the two sets of preditions & compare
auc1 <- auc(actual = credit_test$default, predicted = preds1)  #OOB
auc2 <- auc(actual = credit_test$default, predicted = preds2)  #CV 

# Compare AUC 
print(paste0("Test set AUC (OOB): ", auc1))                         
print(paste0("Test set AUC (CV): ", auc2))
```

# Model Comparison via ROC Curve and AUC

Used to compare all trees
AUC - good for binary classification models
Best AUC means best-performing models

## Exercises

Compare all models based on AUC
In this final exercise, we will perform a model comparison across all types of models that we've learned about so far: Decision Trees, Bagged Trees, Random Forest and Gradient Boosting Machine (GBM). The models were all trained on the same training set, credit_train, and predictions were made for the credit_test dataset.

We have pre-loaded four sets of test set predictions, generated using the models we trained in previous chapters (one for each model type). The numbers stored in the prediction vectors are the raw predicted values themselves -- not the predicted class labels. Using the raw predicted values, we can calculate test set AUC for each model and compare the results.

```{r}
# Generate the test set AUCs using the two sets of predictions & compare
actual <- credit_test$default
dt_auc <- auc(actual = actual, predicted = dt_preds)
bag_auc <- auc(actual = actual, predicted = bag_preds)
rf_auc <- auc(actual = actual, predicted = rf_preds)
gbm_auc <- auc(actual = actual, predicted = gbm_preds)

# Print results
sprintf("Decision Tree Test AUC: %.3f", dt_auc)
sprintf("Bagged Trees Test AUC: %.3f", bag_auc)
sprintf("Random Forest Test AUC: %.3f", rf_auc)
sprintf("GBM Test AUC: %.3f", gbm_auc)
```

Random Forest performed the best on this tests set. Tuning GBM would make the performance closer to random Forest. To save time, we only used 2-fold cross-validation to choose optimal number of trees for GBM but more folds should be used in practice (5-10). More folds will have a better estaimate of the optimal number of trees.

Plot & compare ROC curves
We conclude this course by plotting the ROC curves for all the models (one from each chapter) on the same graph. The ROCR package provides the prediction() and performance() functions which generate the data required for plotting the ROC curve, given a set of predictions and actual (true) values.

The more "up and to the left" the ROC curve of a model is, the better the model. The AUC performance metric is literally the "Area Under the ROC Curve", so the greater the area under this curve, the higher the AUC, and the better-performing the model is.

```{r}
# List of predictions
preds_list <- list(dt_preds, bag_preds, rf_preds, gbm_preds)

# List of actual values (same for all)
m <- length(preds_list)
actuals_list <- rep(list(credit_test$default), m)

# Plot the ROC curves
pred <- prediction(preds_list, actuals_list)
rocs <- performance(pred, "tpr", "fpr")
plot(rocs, col = as.list(1:m), main = "Test Set ROC Curves")
legend(x = "bottomright", 
       legend = c("Decision Tree", "Bagged Trees", "Random Forest", "GBM"),
       fill = 1:m)
```