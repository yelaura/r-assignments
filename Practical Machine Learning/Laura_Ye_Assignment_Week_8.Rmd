---
title: "Assignment Week 8"
author: "Laura Ye"
date: "March 11, 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

## Goals and Purpose

In this assignment, we will use a banking telemarketing data set (Title: Bank Marketing) to build a model that determines whether a customer will subscribe to a bank term deposit. We will be using logistic regression, decision trees, and random forest models to generate the model. The performance of each model will be evaluated and compared using the confusion matrix, ROC and AUC, to select the best model.

## About the data set

To take an initial look at the data set, let's load it into our workspace.

```{r}
library(tidyverse)
bank_data <- read_delim("bank-additional-full.csv", ";")
dim(bank_data)
names(bank_data)
```

There are over 40,000 rows of observations in this data set, with 20 predictor variables and one dependent variable, "y".

### Missing Data

Let's further examine the data set and start by examining the missing data.

```{r}
print (paste0("Number of NAs in the whole data set: ", sum(is.na(bank_data))))

for (i in names(bank_data)){
  print (paste0("Number of NAs in ", i, " column: ", sum(is.na(bank_data %>% select(i)))))
}
```

All of the NAs reside in the "nr.employed" column. This column indicates number of employees for the client.

```{r}
table(bank_data$nr.employed)
```

The column also only has one value, so values from this column would most likely not be useful for our model. This column will be removed.

```{r}
bank_data_clean <- bank_data %>% select(-nr.employed)
```

#### "Unknown" Data

This data set also has a lot of values that are "unknown" which should be treated as NAs.

Let's see how many values are "unknown" after replacing these values with NAs.

```{r}
bank_data_clean[bank_data_clean == "unknown"] <- NA

print (paste0("Number of NAs in the whole data set: ", sum(is.na(bank_data_clean))))

for (i in names(bank_data_clean)){
  print (paste0("% of NAs in ", i, " column: ", round(sum(is.na(bank_data_clean %>% select(i)))/nrow(bank_data_clean)*100, 1)))
}
```

The NAs/Unknowns are congregated in the job, marital, education, default, housing, and loan columns.

For now, we will leave the NAs and preProcess them later while training the models.

Next, let's convert the dependent variable, y, to 0s and 1s.

```{r}
# Convert y to 0 or 1
bank_data_clean$y <- ifelse(bank_data_clean$y == "yes", 1, 0)
```

## Split Data

Since this data set is relatively large, we will split up 80% of the data set into a training set, and 20% of the data set into a test data.

```{r}
library(caret)
set.seed(620)
train_index <- createDataPartition(bank_data_clean$y,
                                   p=0.8,
                                   list=FALSE,
                                   times=1)

train <- bank_data_clean[train_index,]
test <- bank_data_clean[-train_index,]

nrow(train)/nrow(bank_data_clean)
```

Now that we have a train and test set, we are ready to build three different types of models to compare and evaluate their performance.

## Logistic Regression

With logistic regression,

- always start with median imputation; also try knn if data not missing at random
- for linear models: center and scale
- for linear models: try pca and spatialsign
- tree based dont need preprocessing; usually median imputation is enough
- nzv and zv to remove zero variance columns

## Decision Trees (with hyperparameter optimization)

For the decision tree, we will train an initial classification tree model, and then tune the classification tree model by "pruning" the tree and using grid search.

First, let's train a classification tree model.

```{r}
library(rpart)
library(rpart.plot)

class_tree_model <- rpart(formula = y ~ .,
                          data = train,
                          method = "class")

print(class_tree_model)

rpart.plot(class_tree_model, yesno = 2, type = 0, extra = 0)
```

This gives us a nice initial classification tree which is what we'll use to tune the model.

### Tuning the model

We will use grid search now with minsplit, maxdepth, and CP to tune this model.

```{r}

```

## Random Forest



## Summary