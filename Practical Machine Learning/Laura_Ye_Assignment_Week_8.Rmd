---
title: "Assignment Week 8"
author: "Laura Ye"
date: "March 11, 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)

# Load required packages
library(tidyverse)
library(ModelMetrics)
library(caret)
library(rpart)
library(rpart.plot)
library(randomForest)
library(caTools)
library(pROC)
```

## Goals and Purpose

In this assignment, we will use a banking telemarketing data set (Title: Bank Marketing) to build a model that determines whether a customer will subscribe to a bank term deposit. We will be using logistic regression, decision trees, and random forest models to generate the model. The performance of each model will be evaluated and compared using the confusion matrix, ROC and AUC, to select the best model.

## About the data set

To take an initial look at the data set, let's load it into our workspace.

```{r}
bank_data <- read_delim("bank-additional-full.csv", ";")
dim(bank_data)
names(bank_data)
```

There are over 40,000 rows of observations in this data set, with 20 predictor variables and one dependent variable, "y".

### Missing Data

Let's further examine the data set and start by examining the missing data.

```{r}
print (paste0("Number of NAs in the whole data set: ", sum(is.na(bank_data))))

for (i in names(bank_data)){
  if (sum(is.na(bank_data %>% select(i)))){
    print (paste0("Number of NAs in ", i, " column: ", sum(is.na(bank_data %>% select(i)))))
  }
}
```

All of the NAs reside in the "nr.employed" column. This column indicates number of employees for the client.

```{r}
table(bank_data$nr.employed)
```

The column also only has one value, so values from this column would most likely not be useful for our model. This column will be removed.

```{r}
bank_data <- bank_data %>% select(-nr.employed)
```

### Factorize Categorical Variables

There are some columns that need to be converted to a factor so that the models can recognize them as categorical variables.

```{r}
# list of column names to factorize
(cat_cols <- c("job", "marital", "education", "default", "housing", "loan", "contact", "month", "day_of_week", "poutcome", "y"))

for (i in cat_cols){
  bank_data[[i]] <- factor(bank_data[[i]])
}
```


### Near-zero variance and Zero variance

We will now use caret's preProcess function to remove near-zero variance and zero variance explanatory variables.

```{r}
prep_bank_data <- preProcess(bank_data, method=c("zv", "nzv"))
bank_data_clean <- predict(prep_bank_data, bank_data)
```

Comparing the dimensions of the original data set with the preProcessed data set, it looks like one column was removed.

```{r}
dim(bank_data)
dim(bank_data_clean)
```

Which column was removed?

```{r}
setdiff(names(bank_data), names(bank_data_clean))
summary(bank_data$pdays)
```

Since this column's only value was 999, the predictor variable has zero variance and hence, is not useful to this analysis.

### Split Data

Since this data set is relatively large, we will split up 70% of the data set into a training set, and 15% into a validation set, and the rest 15% into a test set.

```{r}
set.seed(620)
assignment <- sample(1:3, size = nrow(bank_data_clean), prob = c(0.7,0.15,0.15), replace = TRUE)

train <- bank_data_clean[assignment == 1, ]
valid <- bank_data_clean[assignment == 2, ]
test <- bank_data_clean[assignment == 3, ]

print("Percentage of rows in training set:")
round(nrow(train)/nrow(bank_data_clean)*100, 2)
```

Now that we have a train, validation, and test set, we are ready to build three different types of models to compare and evaluate their performance.

## Logistic Regression

For logistic regression, we will use the training set to fit a general logistic regression model and then evaluate using the test set. For logistic regression, we will not be using the validation set since there are no parameters to tune.

Let's fit a logistic regression model.

```{r}
# Generate glm model using train set
glm_final <- glm(y~., train, family=binomial)

# Predict new values using test set
glm_pred <- predict(glm_final, newdata=test, type="response")

# Convert response values to yes and no
glm_pred <- factor(ifelse(glm_pred > 0.5, "yes", "no"))

(glm_auc <- ModelMetrics::auc(test$y, glm_pred))
```

The rest of the model metrics will be calculated in the last section, Summary, as a comparison with the other models.

## Decision Trees (with hyperparameter optimization)

For the decision tree, we will train an initial classification tree model, and then tune the classification tree model using grid search.

Let's train a basic classification tree model.

```{r}
set.seed(620)
class_tree_model <- rpart(formula = y ~ .,
                          data = train,
                          method = "class")

rpart.plot(class_tree_model, yesno = 2, type = 0, extra = 0)
```

This gives us a nice initial classification tree which is what we'll use to tune the model.

### Tuning the model

The three hyperparameters we'll take a look at will be cp, minsplit, and max depth.

#### cp

First, let's take a look at what the ideal value for cp should be. 

```{r}
(class_tree_model$cptable)
```

Since the lowest x-val relative error is already at the lowest and default cp value, this model does not need to be pruned based on cp value.

#### minsplit and maxdepth

Next, we will use grid search with minsplit and maxdepth to further tune this model.

```{r}
# establish hyperparameter ranges
minsplit <- seq(1, 20, 1)
maxdepth <- seq(1, 30, 1)

# create a dataframe encompassing all the possible combinations
hyper_grid <- expand.grid(minsplit = minsplit, maxdepth = maxdepth)

print(paste0("Total combinations: ", nrow(hyper_grid)))
```

Next, let's define a function that will return a list of classification tree models based on the parameters in our hyperparameter grid.

```{r}
class_tree_model <- function (parameters){
  models <- list()
  
  for (i in 1:nrow(parameters)){
    models[[i]] <- rpart(formula = y ~ .,
                         data = train,
                         method = "class",
                         minsplit = parameters$minsplit[i],
                         maxdepth = parameters$maxdepth[i])
  }
  
  return (models)
  
}
```

Using this function, I just need to pass the hyperparameter grid to generate the list of models that we're comparing.

```{r, cache=TRUE}
tuning_models <- class_tree_model(hyper_grid)
print(length(tuning_models))
```

The resulting list should contain 600 models, with which we can evaluate using the validation set to determine the best combination of hyperparameters. To evaluate, we will use AUC so that the model with the highest AUC will be chosen.

Like how the list of models was generated using a function, I will now also use a function that takes in a list of models, and the function name of the metric used to evaluate these models.

```{r, cache=TRUE}
compare_models <- function(list_of_models, metric_fun){
  results <- c()
  
  for (i in 1:length(list_of_models)){
    predicted <- predict(list_of_models[[i]], valid, type="vector")
    results[i] <- round(metric_fun(valid$y, predicted), 4)
  }
  
  return(results)
}

class_metric <- compare_models(tuning_models, ModelMetrics::auc)

table(class_metric)
```

AUC for these models are very similar. 10% of the models have the better performing AUC. Let's select these and take a look at the hyperparameters used.

```{r}
auc_max <- max(class_metric)
head(hyper_grid[class_metric==auc_max,])
```

From the AUC value, a maxdepth of 3 is ideal and value of minsplit is not significant, so we will keep that at the default value.

### Final Evaluation

Finally, we are ready to train this tuned classification tree model against the test set.

```{r}
class_final <- rpart(formula = y ~ .,
                     data = train,
                     method = "class",
                     maxdepth = 3)

class_pred <- factor(predict(class_final, test, type="vector"))
levels(class_pred) <- c("no", "yes")

(class_auc <- ModelMetrics::auc(test$y, class_pred))
```

## Random Forest

For the random forest model, we will train a randomForest model using the default values, and then tune the randomForest model using mtry, nodesize, and sampsize.

Let's start by generating a randomForest model using the default parameter values.

```{r}
set.seed(620)

rf_model <- randomForest(y~., train)

print(rf_model)
```

### Tuning the model

The initial randomForest model was not bad, but let's try to tune using mtry, nodesize, and sampsize. Note that in this section, the validation test set will be used to select the best hyperparameters.

To tune these parameters, we will use a hyperparameter grid and extract the OOB error for each model. The parameters for the model with the lowest OOB error will be used for the final evaluation against the test set.

```{r, cache=TRUE}
# Establish a list of possible values for mtry, nodesize and sampsize
mtry <- seq(2,8,2)
nodesize <- seq(2, 8, 2)
sampsize <- nrow(valid) * c(0.7, 0.8)

# Create a data frame containing all combinations 
hyper_grid <- expand.grid(mtry=mtry, nodesize = nodesize, sampsize = sampsize)

# Create an empty vector to store OOB error values
oob_err <- c()

# Write a loop over the rows of hyper_grid to train the grid of models
for (i in 1:nrow(hyper_grid)) {

    # Train a Random Forest model
    model <- randomForest(formula = y ~ ., 
                          data = valid,
                          mtry = hyper_grid$mtry[i],
                          nodesize = hyper_grid$nodesize[i],
                          sampsize = hyper_grid$sampsize[i])
                          
    # Store OOB error for the model                      
    oob_err[i] <- model$err.rate[nrow(model$err.rate), "OOB"]
}

# Identify optimal set of hyperparmeters based on OOB error
opt_i <- which.min(oob_err)
print(hyper_grid[opt_i,])
```

### Final Evaluation

Now that we have the ideal parameters for mtry, nodesize, and sampsize, let's put this together in a model and evaluate the AUC for this model.

```{r}
rf_final <- randomForest(formula = y ~ ., 
                          data = test,
                          mtry = hyper_grid$mtry[opt_i],
                          nodesize = hyper_grid$nodesize[opt_i],
                          sampsize = hyper_grid$sampsize[opt_i])

print(rf_model)

rf_pred <- predict(rf_final, test, type="response")

(rf_auc <- ModelMetrics::auc(test$y, rf_pred))
```

## Summary

In this section, all three models will be compared to each other using confusionMatrix, ROC, and AUC.

Since we've been using AUC to evaluate and compare models while tuning parameters, let's take a look at this metric first.

```{r}
(results <- data.frame(modelname=c("Logistic Regression", "Classification Tree", "Random Forest"), auc=c(glm_auc, class_auc, rf_auc)))
```

Let's also take a look at the confusionMatrix and take a look at the accuracy of each model.

```{r}
glm_cm <- caret::confusionMatrix(glm_pred, test$y)
class_cm <- caret::confusionMatrix(class_pred, test$y)
rf_cm <- caret::confusionMatrix(rf_pred, test$y)

results$accuracy <- c(glm_cm$overall['Accuracy'], class_cm$overall['Accuracy'], rf_cm$overall['Accuracy'])
print(results)
```

From AUC and Accuracy, the random forest model is the most promising since the model yielded the highest values for these metrics.

Finally, let's take a look at the ROC plots for each model.

```{r}
plot.roc(as.numeric(class_pred),as.numeric(test$y), plotROC = TRUE, col="green")
plot.roc(as.numeric(glm_pred),as.numeric(test$y), plotROC = TRUE, add=TRUE, col="blue")
plot.roc(as.numeric(rf_pred),as.numeric(test$y),plotROC = TRUE, add=TRUE, col="red")
plot.roc(as.numeric(test$y),as.numeric(test$y),plotROC = TRUE, add=TRUE, col="black")
legend("bottomright", legend=c("Classification Tree", "Logistic Regression", "Random Forest", "Response Variable"),
       col=c("green", "blue", "red", "black"), lwd=2)
```

The ROC Plot visually confirms that Random Forest model yields the results that are closest to the Response Variable curve, which is just the response variable in the test set plotted against itself in the ROC Plot.

### Conclusion

The random forest model, after tuning for mtry, nodesize, and sampsize, yielded the model with the best results. The accuracy and AUC were the highest of the three models, and the ROC curve was the closest to the ideal curve.

Here's a summary of the Random Forest's performance.

```{r}
print(results[results$modelname=="Random Forest", ])

plot.roc(as.numeric(rf_pred),as.numeric(test$y),plotROC = TRUE, col="red")
plot.roc(as.numeric(test$y),as.numeric(test$y),plotROC = TRUE, add=TRUE, col="black")
legend("bottomright", legend=c("Random Forest", "Response Variable"),
       col=c("red", "black"), lwd=2)
```