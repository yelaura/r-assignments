---
title: "Assignment Week 8"
author: "Laura Ye"
date: "March 11, 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

## Goals and Purpose

In this assignment, we will use a banking telemarketing data set (Title: Bank Marketing) to build a model that determines whether a customer will subscribe to a bank term deposit. We will be using logistic regression, decision trees, and random forest models to generate the model. The performance of each model will be evaluated and compared using the confusion matrix, ROC and AUC, to select the best model.

## About the data set

To take an initial look at the data set, let's load it into our workspace.

```{r}
library(tidyverse)
bank_data <- read_delim("bank-additional-full.csv", ";")
dim(bank_data)
names(bank_data)
```

There are over 40,000 rows of observations in this data set, with 20 predictor variables and one dependent variable, "y".

### Missing Data

Let's further examine the data set and start by examining the missing data.

```{r}
print (paste0("Number of NAs in the whole data set: ", sum(is.na(bank_data))))

for (i in names(bank_data)){
  if (sum(is.na(bank_data %>% select(i)))){
    print (paste0("Number of NAs in ", i, " column: ", sum(is.na(bank_data %>% select(i)))))
  }
}
```

All of the NAs reside in the "nr.employed" column. This column indicates number of employees for the client.

```{r}
table(bank_data$nr.employed)
```

The column also only has one value, so values from this column would most likely not be useful for our model. This column will be removed.

```{r}
bank_data <- bank_data %>% select(-nr.employed)
```

### Factorize Categorical Variables

There are some columns that need to be converted to a factor so that the models can recognize them as categorical variables.

```{r}
# list of column names to factorize
(cat_cols <- c("job", "marital", "education", "default", "housing", "loan", "contact", "month", "day_of_week", "poutcome", "y"))

for (i in cat_cols){
  bank_data[[i]] <- factor(bank_data[[i]])
}
```


### Near-zero variance and Zero variance

We will now use caret's preProcess function to remove near-zero variance and zero variance explanatory variables.

```{r}
library(caret)
prep_bank_data <- preProcess(bank_data, method=c("zv", "nzv"))
bank_data_clean <- predict(prep_bank_data, bank_data)
```

Comparing the dimensions of the original data set with the preProcessed data set, it looks like one column was removed.

```{r}
dim(bank_data)
dim(bank_data_clean)
```

Which column was removed?

```{r}
setdiff(names(bank_data), names(bank_data_clean))
```

### Split Data

Since this data set is relatively large, we will split up 70% of the data set into a training set, and 15% into a validation set, and the rest 15% into a test set.

```{r}
set.seed(620)
assignment <- sample(1:3, size = nrow(bank_data_clean), prob = c(0.7,0.15,0.15), replace = TRUE)

train <- bank_data_clean[assignment == 1, ]
valid <- bank_data_clean[assignment == 2, ]
test <- bank_data_clean[assignment == 3, ]

print("Percentage of rows in training set:")
nrow(train)/nrow(bank_data_clean)
```

Now that we have a train, validation, and test set, we are ready to build three different types of models to compare and evaluate their performance.

## Logistic Regression

With logistic regression,

- always start with median imputation; also try knn if data not missing at random
- for linear models: center and scale
- for linear models: try pca and spatialsign
- tree based dont need preprocessing; usually median imputation is enough
- nzv and zv to remove zero variance columns

## Decision Trees (with hyperparameter optimization)

For the decision tree, we will train an initial classification tree model, and then tune the classification tree model using grid search.

Let's train a basic classification tree model.

```{r}
library(rpart)
library(rpart.plot)

set.seed(620)
class_tree_model <- rpart(formula = y ~ .,
                          data = train,
                          method = "class")

print(class_tree_model)

rpart.plot(class_tree_model, yesno = 2, type = 0, extra = 0)
```

This gives us a nice initial classification tree which is what we'll use to tune the model.

### Tuning the model

The three hyperparameters we'll take a look at will be cp, minsplit, and max depth.

#### cp

First, let's take a look at what the ideal value for cp should be. 

```{r}
(class_tree_model$cptable)
```

Since the lowest x-val relative error is already at the lowest and default cp value, this model does not need to be pruned based on cp value.

#### minsplit and maxdepth

Next, we will use grid search with minsplit and maxdepth to further tune this model.

```{r}
# establish hyperparameter ranges
minsplit <- seq(1, 20, 1)
maxdepth <- seq(1, 30, 1)

# create a dataframe encompassing all the possible combinations
hyper_grid <- expand.grid(minsplit = minsplit, maxdepth = maxdepth)

print(paste0("Total combinations: ", nrow(hyper_grid)))
```

Next, let's define a function that will return a list of classification tree models based on the parameters in our hyperparameter grid.

```{r}
class_tree_model <- function (parameters){
  models <- list()
  
  for (i in 1:nrow(parameters)){
    models[[i]] <- rpart(formula = y ~ .,
                         data = train,
                         method = "class",
                         minsplit = parameters$minsplit[i],
                         maxdepth = parameters$maxdepth[i])
  }
  
  return (models)
  
}
```

Using this function, I just need to pass the hyperparameter grid to generate the list of models that we're comparing.

```{r}
tuning_models <- class_tree_model(hyper_grid)
print(length(tuning_models))
```

The resulting list should contain 360 models, with which we can evaluate using the validation set to determine the best combination of hyperparameters. To evaluate, we will use AUC so that the model with the highest AUC will be chosen.

Like how the list of models was generated using a function, I will now also use a function that takes in a list of models, and the function name of the metric used to evaluate these models.

```{r}
library(ModelMetrics)

compare_models <- function(list_of_models, metric_fun){
  results <- c()
  
  for (i in 1:length(list_of_models)){
    predicted <- predict(list_of_models[[i]], valid, type="vector")
    results[i] <- round(metric_fun(valid$y, predicted), 4)
  }
  
  return(results)
}

class_metric <- compare_models(tuning_models, auc)

table(class_metric)
```

AUC for these models are very similar. 10% of the models have the better performing AUC. Let's select these and take a look at the hyperparameters used.

```{r}
auc_max <- max(class_metric)
head(hyper_grid[class_metric==auc_max,])
```

From the AUC value, a maxdepth of 3 is ideal and value of minsplit is not significant, so we will keep that at the default value.

### Final Evaluation

Finally, we are ready to train this tuned classification tree model against the test set.

```{r}
class_final <- rpart(formula = y ~ .,
                     data = train,
                     method = "class",
                     maxdepth = 3)

class_pred <- predict(class_final, test, type="vector")

(class_auc <- auc(test$y, class_pred))
```

## Random Forest

For the random forest model, we will train a randomForest model using the default values, and then tune the randomForest model using mtry, nodesize, and sampsize.

Let's start by generating a randomForest model using the default parameter values.

```{r}
set.seed(620)

library(randomForest)
rf_model <- randomForest(y~., train)

print(rf_model)
```

### Tuning the model

The initial randomForest model was not bad, but let's try to tune using mtry, nodesize, and sampsize. Note that in this section, the validation test set will be used to select the best hyperparameters.

To tune these parameters, we will use a hyperparameter grid and extract the OOB error for each model. The parameters for the model with the lowest OOB error will be used for the final evaluation against the test set.

```{r}
# Establish a list of possible values for mtry, nodesize and sampsize
mtry <- seq(2,8,2)
nodesize <- seq(2, 8, 2)
sampsize <- nrow(valid) * c(0.7, 0.8)

# Create a data frame containing all combinations 
hyper_grid <- expand.grid(mtry=mtry, nodesize = nodesize, sampsize = sampsize)

# Create an empty vector to store OOB error values
oob_err <- c()

# Write a loop over the rows of hyper_grid to train the grid of models
for (i in 1:nrow(hyper_grid)) {

    # Train a Random Forest model
    model <- randomForest(formula = y ~ ., 
                          data = valid,
                          mtry = hyper_grid$mtry[i],
                          nodesize = hyper_grid$nodesize[i],
                          sampsize = hyper_grid$sampsize[i])
                          
    # Store OOB error for the model                      
    oob_err[i] <- model$err.rate[nrow(model$err.rate), "OOB"]
}

# Identify optimal set of hyperparmeters based on OOB error
opt_i <- which.min(oob_err)
print(hyper_grid[opt_i,])
```

### Final Evaluation

Now that we have the ideal parameters for mtry, nodesize, and sampsize, let's put this together in a model and evaluate the AUC for this model.

```{r}
rf_final <- randomForest(formula = y ~ ., 
                          data = test,
                          mtry = hyper_grid$mtry[opt_i],
                          nodesize = hyper_grid$nodesize[opt_i],
                          sampsize = hyper_grid$sampsize[opt_i])

print(rf_model)

rf_pred <- predict(rf_final, test, type="response")

(rf_auc <- auc(test$y, rf_pred))
```

## Summary