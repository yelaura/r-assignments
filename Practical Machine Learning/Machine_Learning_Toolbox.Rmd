---
title: "R Notebook"
output: html_notebook
author: "Laura Ye"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval=FALSE)
```

Notes for DataCamp Module: Machine Learning Toolbox

# Welcome

Supervised Learning
- Target variable

Two Types of predictive models
- Classification (Qualitative)
- Regression (Quantitative)

Metrics
- Quantifiable
- objective
- E.g. RMSE

Common to calculate in-sample RMSE but too optimistic and leads to over fitting.
- Better to calculate out of sample error (more real world and prevents over-fitting)

In Sample Prediction (Prediction on Training Data)

## Exercises

As you saw in the video, included in the course is the diamonds dataset, which is a classic dataset from the ggplot2 package. The dataset contains physical attributes of diamonds as well as the price they sold for. One interesting modeling challenge is predicting diamond price based on their attributes using something like a linear regression.

```{r}
# Fit lm model: model
model <- lm(price ~ ., diamonds)

# Predict on full data: p
p <- predict(model, diamonds)

# Compute errors: error
error <- p - diamonds$price

# Calculate RMSE
sqrt(mean(error^2))
```

# Out of Sample Error Measures

Predictive vs Explanatory Data Modeling
- Prefer models that do not overfit and generalize well
- Do models perform well on new data?
- Must test models on a test set that was not exposed to the model

In sample validation guarantees overfitting

Primary goal of caret and this course: Don't overfit

Compare to in-sample RMSE and out-of-sample RMSE is pretty bad

## Exercises

One way you can take a train/test split of a dataset is to order the dataset randomly, then divide it into the two sets. This ensures that the training set and test set are both random samples and that any biases in the ordering of the dataset (e.g. if it had originally been ordered by price or size) are not retained in the samples we take for training and testing your models. You can think of this like shuffling a brand new deck of playing cards before dealing hands.

```{r}
# Set seed
set.seed(42)

# Shuffle row indices: rows
rows <- sample(nrow(diamonds))

# Randomly order data
diamonds <- diamonds[rows, ]
```

Try an 80/20 split
Now that your dataset is randomly ordered, you can split the first 80% of it into a training set, and the last 20% into a test set.

```{r}
# Determine row to split on: split
split <- round(nrow(diamonds) * .80)

# Create train
train <- diamonds[1:split, ]

# Create test
test <- diamonds[(split + 1):nrow(diamonds), ]
```

Predict on test set
Now that you have a randomly split training set and test set, you can use the lm() function as you did in the first exercise to fit a model to your training set, rather than the entire dataset. 

```{r}
# Fit lm model on train: model
model <- lm(price ~ ., train)

# Predict on test: p
p <- predict(model, test)
```

Calculate test set RMSE by hand
Now that you have predictions on the test set, you can use these predictions to calculate an error metric (in this case RMSE) on the test set and see how the model performs out-of-sample, rather than in-sample as you did in the first exercise. You first do this by calculating the errors between the predicted diamond prices and the actual diamond prices by subtracting the predictions from the actual values.

```{r}
# Compute errors: error
error <- p-test$price

# Calculate RMSE
sqrt(mean(error^2))
```

# Cross-Validation

Outliers can heavily affect train/test sets

Multiple test sets: e.g. cross-validation
- Each "fold" contains a unique set of observations (no replacement)

Reduces systematic biases in the model

Only used to estimate out-of-sample error for test model
- After you're done, you use the whole training set to model
- Best with little data

train(formula, data, method, trControl)
- method: can be lm or anything in caret
- trControl: parameters for cross-validation methods (method="cv")

## Exercises

10-fold cross-validation
As you saw in the video, a better approach to validating models is to use multiple systematic test sets, rather than a single random train/test split.

```{r}
# Fit lm model using 10-fold CV: model
model <- train(
  price ~ . , diamonds,
  method = "lm",
  trControl = trainControl(
    method = "cv", number = 10,
    verboseIter = TRUE
  )
)

# Print model to console
print(model)
```

5-fold cross-validation
In this course, you will use a wide variety of datasets to explore the full flexibility of the caret package. Here, you will use the famous Boston housing dataset, where the goal is to predict median home values in various Boston suburbs.

```{r}
# Fit lm model using 5-fold CV: model
model <- train(
  medv ~ ., Boston,
  method = "lm",
  trControl = trainControl(
    method = "cv", number = 5,
    verboseIter = TRUE
  )
)

# Print model to console
print(model)
```

5 x 5-fold cross-validation
You can do more than just one iteration of cross-validation. Repeated cross-validation gives you a better estimate of the test-set error. You can also repeat the entire cross-validation procedure. This takes longer, but gives you many more out-of-sample datasets to look at and much more precise assessments of how well the model performs.

```{r}
# Fit lm model using 5 x 5-fold CV: model
model <- train(
  medv ~ ., Boston,
  method = "lm",
  trControl = trainControl(
    method = "cv", number = 5,
    repeats = 5, verboseIter = TRUE
  )
)

# Print model to console
print(model)
```

Making predictions on new data
Finally, the model you fit with the train() function has the exact same predict() interface as the linear regression models you fit earlier in this chapter.

```{r}
# Predict on full Boston dataset
predict(model, Boston)
```

# Logistic Regression on sonar

Classification models
- Categorical target variable
- Supervised learning
- train/test split
- Use Sonar dataset
- Goal: R or M?

Analyzing sonar signals was one of the early applications of machine learning

Sonar dataset is small, so 60/40 gives a larger test set

## Exercises

Try a 60/40 split
As you saw in the video, you'll be working with the Sonar dataset in this chapter, using a 60% training set and a 40% test set. We'll practice making a train/test split one more time, just to be sure you have the hang of it. 

```{r}
# Shuffle row indices: rows
rows <- sample(nrow(Sonar))

# Randomly order data: Sonar
Sonar <- Sonar[rows, ]

# Identify row to split on: split
split <- round(nrow(Sonar) * 0.6)

# Create train
train <- Sonar[1:split, ]

# Create test
test <- Sonar[(split+1):nrow(Sonar), ]
```

Fit a logistic regression model
Once you have your random training and test sets you can fit a logistic regression model to your training set using the glm() function. glm() is a more advanced version of lm() that allows for more varied types of regression models, aside from plain vanilla ordinary least squares regression.

```{r}
# Fit glm model: model
model <- glm(Class ~., train, family="binomial")

# Predict on test: p
p <- predict(model, test, type="response")
```

# Confusion matrix

Matrix of the model's predicted classes and the actual outcomes
- Shows how confused the model is

Train, predict, interpret probabilities, compare, confusion matrix

Accuracy - most useful statistics

No Information Rate - dummy model that always predicts the majority

## Exercises

Calculate a confusion matrix
As you saw in the video, a confusion matrix is a very useful tool for calibrating the output of a model and examining all possible outcomes of your predictions (true positive, true negative, false positive, false negative).

```{r}
# Calculate class probabilities: p_class
p_class <- ifelse(p > 0.50, "M", "R")

# Create confusion matrix
confusionMatrix(p_class, test[["Class"]])
```

# Class probabilities and class predictions

Different thresholds
- Not limited to 50% threshold
- Balance true positive and false positive rates
- Cost benefit analysis
- No good heuristic

## Exercises

Try another threshold
In the previous exercises, you used a threshold of 0.50 to cut your predicted probabilities to make class predictions (rock vs mine). However, this classification threshold does not always align with the goals for a given modeling problem.

```{r}
# Apply threshold of 0.9: p_class
p_class <- ifelse(p>0.9, "M", "R")

# Create confusion matrix
confusionMatrix(p_class, test[["Class"]])
```

From probabilites to confusion matrix
Conversely, say you want to be really certain that your model correctly identifies all the mines as mines. In this case, you might use a prediction threshold of 0.10, instead of 0.90.

```{r}
# Apply threshold of 0.10: p_class
p_class <- ifelse(p > 0.10, "M", "R")

# Create confusion matrix
confusionMatrix(p_class, test[["Class"]])
```

# Introducing ROC curve

Challenges
- many possible classification thresholds
- requires manual work to choose
- easy to overlook
- need a systematic approach

ROC Curves
- Plot true/false positive rate at every possible threshold
- Visualize tradeoffs in both extremes

Example
```{r}
library(caTools)
colAUC(p, test[["Class"]], plotROC=TRUE)
```

## Exercises

Plot an ROC curve
As you saw in the video, an ROC curve is a really useful shortcut for summarizing the performance of a classifier over all possible thresholds. This saves you a lot of tedious work computing class predictions for many different thresholds and examining the confusion matrix for each.

My favorite package for computing ROC curves is caTools, which contains a function called colAUC(). This function is very user-friendly and can actually calculate ROC curves for multiple predictors at once.

```{r}
# Predict on test: p
p <- predict(model, test, type="response")

# Make ROC curve
colAUC(p, test[["Class"]], plotROC=TRUE)
```

# Area under the curve

Models with random predictions are close to the x=y line
Models with threshold that allows perfect separation follows the x=0 and y=1 box

Area under the curve for a perfect model is 1.0
Average area under the curve for a model is 0.50

Defining AUC (Area under curve)
- based on ROC Curve
- Single number summary of model accuracy
- Summarizes across all thresholds
- Ranges from 0 to 1 (1 meaning model is always right)
- Rule of thumb: AUC is a letter grade

## Exercises

Customizing trainControl
As you saw in the video, area under the ROC curve is a very useful, single-number summary of a model's ability to discriminate the positive from the negative class (e.g. mines from rocks). An AUC of 0.5 is no better than random guessing, an AUC of 1.0 is a perfectly predictive model, and an AUC of 0.0 is perfectly anti-predictive (which rarely happens).

```{r}
# Create trainControl object: myControl
myControl <- trainControl(
  method = "cv",
  number = 10,
  summaryFunction = twoClassSummary,
  classProbs = TRUE, # IMPORTANT!
  verboseIter = TRUE
)
```

Using custom trainControl
Now that you have a custom trainControl object, it's easy to fit caret models that use AUC rather than accuracy to tune and evaluate the model. You can just pass your custom trainControl object to the train() function via the trControl argument.

```{r}
# Train glm with custom trainControl: model
model <- train(Class ~ ., Sonar, method="glm", trControl=myControl)

# Print model to console
print(model)
```

