---
title: "Inference for Linear Regression - Data Camp"
output: html_notebook
author: "Laura Ye"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval=FALSE)
```

Notes for DataCamp Module: Inference for Linear Regression

# Variability in regression lines

Inferential instead of descriptive claims about models
using least squares estimation
create confidence intervals for the slope

subset of fewer observations show a positive linear trend
least squares regression line are not identical
- variability in sample to sample

sampling variability
variability in regression line
50 samples, 50 different lines

rep_sample_n
density plot showing distribution of the slope

broom package - pull out slope from the linear fit
slopes between 8 and 17
nowhere close to zero
so there is definitely a positive correlation

## Exercises

Regression output: example I
The following code provides two equivalent methods for calculating the most important pieces of the linear model output. Recall that the p-value is the probability of the observed data (or more extreme) given the null hypothesis is true. As with inference in other settings, you will need the sampling distribution for the statistic (here the slope) assuming the null hypothesis is true. You will generate the null sampling distribution in later chapters, but for now, assume that the null sampling distribution is correct. Additionally, notice that the standard error of the slope and intercept estimates describe the variability of those estimates.

```{r}
# Load the mosaicData package and the RailTrail data
library(mosaicData)
data(RailTrail)

# Fit a linear model
ride_lm <- lm(volume ~ hightemp, RailTrail)

# View the summary of your model
summary(ride_lm)

# Print the tidy model output
ride_lm %>% tidy()
```

First random sample, second random sample
Now, you will dive in to understanding how linear models vary from sample to sample. Here two random samples from a population are plotted onto the scatterplot. The population data (called popdata) already exists and is pre-loaded, along with ggplot and dplyr.

```{r}
# Plot the whole dataset
ggplot(popdata, aes(x = explanatory, y = response)) + 
  geom_point() + 
  geom_smooth(method = "lm", se = FALSE) 

# Take 2 samples of size 50
set.seed(4747)
sample1 <- popdata %>%
  sample_n(size = 50)

sample2 <- popdata %>%
  sample_n(size = 50)

# Plot sample1
plot1 <- ggplot(sample1, aes(x = explanatory, y = response)) + 
  geom_point(color = "blue") + 
  geom_smooth(method = "lm", se = FALSE, color = "blue")

plot1

# Plot sample2 over sample1
plot1 + geom_point(data = sample2, 
                   aes(x = explanatory, y = response),
                   color = "red") + 
  geom_smooth(data = sample2, 
              aes(x = explanatory, y = response), 
              method = "lm", 
              se = FALSE, 
              color = "red")
```

Superimpose lines
Building on the previous exercise, you will now repeat the process 100 times in order to visualize the sampling distribution of regression lines generated by 100 different random samples of the population.

```{r}
# Repeatedly sample the population
manysamples <- rep_sample_n(popdata, 50, reps=100)

# Plot the regression lines
ggplot(manysamples, aes(x= explanatory, y= response, group=replicate)) + 
  geom_point() + 
  geom_smooth(method = "lm", se = FALSE) 

# Fit and tidy many linear models
manylms <- manysamples %>% 
  group_by(replicate) %>% 
  do(lm(response ~ explanatory, data=.)  %>% 
     tidy()) %>%
  filter(term=="explanatory")

# Plot a histogram of the slope coefficients
ggplot(manylms, aes(x=estimate)) +
  geom_histogram()
```

# Research question

Is there a linear model in the entire population of foods from starbucks?

Two sided research question - are protein and carbs linearly associated in the population?
One sided research question - two variables linearly associated in a positive direction in the population?

summary(model) vs tidy(model)
- format is different
- content is the same
- Std. error = std.error
- statistic = t.value (measures number of std errors that the estimate is above 0)
- p.value (two-sided) need to divide by 2 for a one-sided hypothesis

# Variability of coefficients

variability between sample and population

smaller sample sizes n=10 vs n=50
- smaller sample sizes have larger variability

less variability in the x-direction
- variability in the slope increases from the original data because high values in the x-direction no longer act as an anchor

## Exercise

Original population - change sample size
In order to understand the sampling distribution associated with the slope coefficient, it is valuable to visualize the impact changes in the sample and population have on the slope coefficient. Here, changing the sample size directly impacts how variable the slope is.

```{r}
# Take 100 samples of size 50
manysamples1 <- rep_sample_n(popdata, 50, reps=100)

# Plot the regression line for each sample
ggplot(manysamples1, aes(x=explanatory, y=response, group=replicate)) + 
  geom_point() + 
  geom_smooth(method = "lm", se = FALSE) 

# Take 100 samples of size 10
manysamples2 <- rep_sample_n(popdata, 10, reps=100)

# Plot the regression line for each sample
ggplot(manysamples2, aes(x=explanatory, y=response, group=replicate)) + 
  geom_point() + 
  geom_smooth(method = "lm", se = FALSE) 
```

Hypothetical population - less variability around the line
In order to understand the sampling distribution associated with the slope coefficient, it is valuable to visualize the impact changes in the sample and population have on the slope coefficient. Here, reducing the variance associated with the response variable around the line changes the variability associated with the slope statistics.

```{r}
# Take 100 samples of size 50
manysamples <- rep_sample_n(popdata, 50, reps=100)

# Plot a regression line for each sample

ggplot(manysamples, aes(x=explanatory, y=response, group=replicate)) + 
  geom_point() + 
  geom_smooth(method = "lm", se = FALSE) 
```

Hypothetical population - less variability in x direction
In order to understand the sampling distribution associated with the slope coefficient, it is valuable to visualize the impact changes in the sample and population have on the slope coefficient. Here, reducing the variance associated with the explanatory variable around the line changes the variability associated with the slope statistics.

```{r}
# Take 100 samples of size 50
manysamples <- rep_sample_n(popdata, 50, reps=100)

# Plot a regression line for each sample
ggplot(manysamples, aes(x=explanatory, y=response, group=replicate)) + 
  geom_point() + 
  geom_smooth(method = "lm", se = FALSE)  +
  xlim(-17, 17)
```

Somewhat counter-intuitively, reducing the variability in the direction of the explanatory variable INCREASES the variability of the slope coefficients. This is because with a smaller range of the explanatory variable, there is less information on which to build the model.

# Simulation-based Inference

Data set is paired between foster and biological (x) twin.
If we permutated the data without regards for pairs, would there still be the same correlation?
Permutated data do not have the same structure as the original data
Two permutated data sets are not identical
- how the lines would vary if there was NO relationship
- if the null hypothesis is true (slope is zero)

use infer package
```{r}
dataset %>% 
  specify (formula) %>%
  hypothesize(null = "independence") %>%
  generate (reps = 10, type="permute") %>%
  calculate (stat = "slope")
```

permutated slope distributions do not fall anywhere near the observed slope
- not possible that it's chance

## Exercises

Null sampling distribution of the slope
In the previous chapter, you investigated the sampling distribution of the slope from a population where the slope was non-zero. Typically, however, to do inference, you will need to know the sampling distribution of the slope under the hypothesis that there is no relationship between the explanatory and response variables. Additionally, in most situations, you don't know the population from which the data came, so the null sampling distribution must be derived from only the original dataset.

In the mid-20th century, a study was conducted that tracked down identical twins that were separated at birth: one child was raised in the home of their biological parents and the other in a foster home. In an attempt to answer the question of whether intelligence is the result of nature or nurture, both children were given IQ tests. The resulting data is given for the IQs of the foster twins (Foster is the response variable) and the IQs of the biological twins (Biological is the explanatory variable).

In this exercise you'll use the pull() function. This function takes a data frame and returns a selected column as a vector (similar to $).

```{r}
# Load the infer package
library(infer)

# Calculate the observed slope
obs_slope <- lm(Foster ~ Biological, data=twins) %>%
  tidy() %>%   
  filter(term == "Biological") %>%
  pull(estimate)    

# Simulate 10 slopes with a permuted dataset
set.seed(4747)
perm_slope <- twins %>%
  specify(Foster ~ Biological) %>%
  hypothesize(null = "independence") %>%
  generate(reps = 10, type = "permute") %>%
  calculate(stat = "slope") 

# Print the observed slope and the 10 permuted slopes
print(obs_slope)
print(perm_slope)
```

SE of the slope
The previous exercise generated 10 different slopes under a model of no (i.e., null) relationship between the explanatory and response variables. Now repeat the null slope calculations 1000 times to derive a null sampling distribution for the slope coefficient. The null sampling distribution will be used as a benchmark against which to compare the original data. calculate the average and SE of the sampling distribution

```{r}
# Make a dataframe with replicates and plot them!
set.seed(4747)
perm_slope <- twins %>%
  specify(Foster ~ Biological) %>%
  hypothesize(null = "independence") %>%
  generate(reps = 500, type = "permute") %>%
  calculate(stat = "slope") 

ggplot(perm_slope, aes(x=stat)) +
  geom_density()

# Calculate the mean and the standard deviation of the slopes
mean(perm_slope$stat)
sd(perm_slope$stat)
```

p-value
Now that you have created the null sampling distribution, you can use it to find the p-value associated with the original slope statistic from the twins data. Although you might first consider this to be a one-sided research question, instead, use the absolute value function for practice performing two-sided tests on a slope coefficient.

```{r}
# Calculate the absolute value of the slope
abs_obs_slope <- lm(Foster ~ Biological, data=twins) %>%
  tidy() %>%   
  filter(term == "Biological") %>%
  pull(estimate) %>%
  abs()

# Compute the p-value  
perm_slope %>% 
  mutate(abs_perm_slope = abs(stat)) %>%
  summarize(p_value = mean(abs_perm_slope > abs_obs_slope))
```

# Simulation-based CI for slope

bootstrapping - resampling method that allows us to estimate the sampling distribution of the statistic in interest (slope)
confidence interval so no null hypothesis - don't need to permute

bootstrap - sample with replacement (some will be left out) - paired data

main difference between permuted and bootstrapped
- slopes from permuted is centered around y=0 and used for hypothesis testing
- slopes from bootstrapped is centered around estimated slope and used for confidence intervals

permutation code 

```{r}
twins %>%
  specify (formula) %>%
  hypothesize (null = "independence") %>%
  generate(reps=100, type="permute") %>%
  calculate(stat= "slope")
```

bootstrap code

```{r}
twins %>%
  specify (formula) %>%
  generate (reps=100, type="bootstrap") %>%
  calculate (stat="slope")
```

## Exercises

Bootstrapping the data
Using the infer package with type="bootstrap", you can repeatedly sample from the dataset to estimate the sampling distribution and standard error of the slope coefficient. Using the sampling distribution will allow you to directly find a confidence interval for the underlying population slope.

```{r}
# Load the infer package
library(infer)

# Calculate 1000 bootstrapped slopes
set.seed(4747)
BS_slope <- twins %>%
  specify(Foster ~ Biological) %>%
  generate(reps = 1000, type = "bootstrap") %>%
  calculate(stat = "slope") 

# Look at the head of BS_slope  
head(BS_slope)
```

SE method - bootstrap CI for slope
The twins study was used to weigh in on the question of whether IQ is a result of nature (your genes) or nurture (your environment). If IQ was purely a result of nature, what slope would you expect to see in your linear model?

Recall that one way to create a confidence interval for a statistic is to calculate the statistic repeatedly under different bootstrap samples and to find the standard deviation associated with the bootstrapped statistics.

```{r}
# Bootstrap the slopes
set.seed(4747)
BS_slope <- twins %>%
  specify(Foster ~ Biological) %>%
  generate(reps = 1000, type = "bootstrap") %>%
  calculate(stat = "slope") 

# Create a confidence interval
BS_slope %>% 
summarize(lower = mean(stat)-2*sd(stat),
          upper = mean(stat)+2*sd(stat))
```

Percentile method - bootstrap CI for slope
Alternatively, a CI for the slope can be created using the percentiles of the distribution of the bootstrapped slope statistics.

```{r}
# Set alpha
alpha <- 0.05

# Bootstrap the slopes
set.seed(4747)
BS_slope <- twins %>%
  specify(Foster ~ Biological) %>%
  generate(reps = 1000, type = "bootstrap") %>%
  calculate(stat = "slope")

# Create a confidence interval  
BS_slope %>% 
summarize(low = quantile(stat, alpha/2), 
          high = quantile(stat, 1- alpha/2))
```

# Mathematical approximation

test and estimate slope parameter
builds on t-distribution

histogram of the bootstrapped slopes shows t-distribution

```{r}
ggplot(starFatCal, aes(x = statistic)) + 
  geom_histogram(aes(y = ..density..), bins = 50) + 
  stat_function(fun = dt, 
                color = "red", 
                args = list(df = nrow(starbucks) - 2))
```

use dt function for density of the t-distribution (not typically used) but it's used in the background

even in the tails, the histogram fits the t-distribution well

poor t fit (red line all over or under the actual distribution)

## Exercises

t-statistic
Using the permuted datasets (recall, the randomization forces the null hypothesis to be true), investigate the distribution of the standardized slope statistics (the slope, which has been divided by the standard error). Note that the distribution of the standardized slope is well described by a t-distribution.

```{r}
# Look at the head of the data
head(twins_perm)

# Plot the histogram with the t distribution
twins_perm %>%
  filter(term == "Biological_perm") %>%
  ggplot(aes(x=statistic)) + 
  geom_histogram(aes(y = ..density..), bins = 50) + 
  stat_function(fun = dt, color = "red", args=list(df=nrow(twins)-2))
```

Working with R-output (1)
The p-value given by the lm output is a two-sided p-value by default. In the twin study, it might seem more reasonable to follow along the one-sided scientific hypothesis that the IQ scores of the twins are positively associated. Because the p-value is the probability of the observed data or more extreme, the two-sided test is twice as big as the one-sided result. That is, to get a one-sided p-value from the two-sided output in R, simply divide the p-value by two.

```{r}
# Tidy the model
lm(Foster ~ Biological, data=twins) %>% tidy()

# Create a one-sided p-value
lm(Foster ~ Biological, data=twins) %>% 
  tidy() %>%
  filter(term=="Biological") %>% 
  select(p.value) %>%
  mutate(p_value_1side = p.value/2)
```

Working with R-output (2)
In thinking about the scientific research question, if IQ is caused only by genetics, then we would expect the slope of the line between the two sets of twins to be 1. Testing the hypothesized slope value of 1 can be done by making a new test statistic which evaluates how far the observed slope is from the hypothesized value of 1.

newt=slope???1SE
If the hypothesis that the slope equals one is true, then the new test statistic will have a t-distribution which we can use for calculating a p-value.

```{r}
# Tidy the model
lm(Foster ~ Biological, data = twins) %>% tidy()

# Test the new hypothesis
lm(Foster ~ Biological, data = twins) %>% 
  tidy() %>% 
  filter(term == "Biological") %>%
  mutate(statistic_test1 = (estimate - 1) / std.error, 
         p_value_test1 = 2 * pt(statistic_test1, df = nrow(twins)-2))
```

Comparing randomization inference and t-inference
When technical conditions (see next chapter) hold, the inference from the randomization test and the t-distribution test should give equivalent conclusions. They will not provide the exact same answer because they are based on different methods. But they should give p-values and confidence intervals that are reasonably close.

```{r}
# Tidy the model
lm(Foster ~ Biological, data=twins) %>% tidy()

# Find the p-value
perm_slope %>%
  mutate(abs_perm_slope = abs(stat)) %>%
  summarize(p_value = mean(abs_perm_slope > abs(obs_slope)))
```

# Intervals in regression

CI is given in tidy evaluation
conf.low and conf.high

## Exercises

CI using t-theory
In previous courses, you have created confidence intervals with the formula of statistic plus/minus some number of standard errors. With bootstrapping, we typically use two standard errors. With t-based theory, we use the specific t-multiplier.

Create a CI for the slope parameter using both the default tidy call as well as mutate to calculate the confidence interval bounds explicitly. Note that the two methods should give exactly the same CI values because they are using the same computations.

```{r}
# Set alpha
alpha <- 0.05

# Find the critical value
crit_val <- qt(0.975, df = nrow(twins)-2)

# Tidy the model with the confidence level alpha
lm(Foster ~ Biological, data=twins) %>% 
   tidy(conf.int=TRUE, conf.level=1-alpha)

# Find the lower and upper bounds of the confidence interval
lm(Foster~Biological, twins) %>%
    tidy() %>%
    mutate(lower = estimate-crit_val*std.error,
        upper = estimate+crit_val*std.error)
```

Comparing randomization CIs and t-based CIs
As with hypothesis testing, if technical conditions hold (technical conditions are discussed more in the next chapter), the CI created for the slope parameter in the t-distribution setting should be in line with the CI created using bootstrapping. Create a CI for the slope parameter and compare it to the one created using the bootstrap percentile interval from the previous chapter.

Note that the bootstrap and t-intervals will not be exactly the same because they use different computational steps to arrive at the interval estimate.

```{r}
# Set alpha
alpha = 0.05

# Tidy the model with confidence intervals and confidence level alpha
lm(Foster ~ Biological, data=twins) %>% 
   tidy(conf.int=TRUE, conf.level=1-alpha)

# Create the bootstrap confidence interval
BS_slope %>% 
    summarize(low = quantile(stat, alpha/2), 
              high = quantile(stat, 1-alpha/2))
```

# Different types of intervals

augment function in broom package can calculate the error
use se=TRUE to plot CI ribbon

## Exercises

Confidence intervals for the average response at specific values
The previous two exercises gave CIs for the slope parameter. That is, based on the observed data, you provided an interval estimate of the plausible values for the true slope parameter in the population. Recall that the number 1 was in the CI for the slope, meaning 1 cannot be ruled out as a possible value for the true slope between Biological and Foster twins. There is no evidence to claim that there is a difference, on average, between the IQ scores of two twins in any given pair.

When working with a linear regression model, you might also want to know the plausible values for the expected value of the response at a particular explanatory location. That is, what would you expect the IQ of a Foster twin to be given a Biological twin's IQ of 100?

```{r}
# Set alpha
alpha <- .05

# Find the critical value
crit_val <- qt((1-alpha/2), df = nrow(twins) - 2)

# Create a dataframe of new observations
newtwins <- data.frame(Biological = c(80,90,100,110))

# Find prediction intervals
lm(Foster ~ Biological, data=twins) %>% 
  augment(newdata = newtwins) %>%
  mutate(lowMean = .fitted - crit_val*.se.fit,
         upMean = .fitted + crit_val*.se.fit)
```

Confidence intervals for the average response for all observations
The confidence interval for the average response can be computed for all observations in the dataset. Using augment directly on the twins dataset gives predictions and standard errors for the Foster twin based on all the Biological observations.

Note that the calculation of the regression line is more stable at the center, so predictions for the extreme values are more variable than predictions in the middle of the range of explanatory IQs.

```{r}
# Set alpha and find the critical value
alpha <- 0.05
crit_val <- qt(1-alpha/2, df=nrow(twins)-2)

# Find confidence intervals for the response
predMeans <- lm(Foster~Biological, data=twins) %>%
  augment() %>%  
  mutate(lowMean = .fitted - crit_val*.se.fit,
      upMean = .fitted + crit_val*.se.fit) 

# Examine the intervals
head(predMeans)

# Plot the data with geom_ribbon()
ggplot(predMeans, aes(x=Biological, y=Foster)) + 
  geom_point() +
  stat_smooth(method="lm", se=FALSE) +
  geom_ribbon(aes(ymin = lowMean, ymax = upMean), alpha=.2)

# Plot the data with stat_smooth()
ggplot(twins, aes(x = Biological, y = Foster)) + 
  geom_point() +
  stat_smooth(method="lm", se=TRUE) 
```

Prediction intervals for the individual response
Along with an interval estimate for the expected value of the response, it is often desired to have an interval estimate for the actual individual responses. The formulation for the prediction is the same, but the predicted points are more variable around the line, so the standard error is calculated to be a larger value.

As with the interval around the expected average values, the interval for predicted individual values is smaller in the middle than on the extremes due to the calculation of the regression line being more stable at the center. Note that the intervals for the average responses are much smaller than the intervals for the individual responses.

```{r}
# Set alpha and find the critical value
alpha <- 0.05
crit_val <- qt(1-alpha/2, df=nrow(twins)-2)

# Fit a model and use glance to find sigma
twin_lm <- lm(Foster~Biological, data=twins)
twin_gl <- glance(twin_lm)

# Pull sigma
twin_sig <- pull(twin_gl, sigma)

# Augment the model to find the prediction standard errors
twin_pred <- augment(twin_lm) %>%
  mutate(.se.pred = sqrt(twin_sig^2 + .se.fit^2))

# Create prediction intervals  
predResp <- twin_pred %>%
  mutate(lowResp = .fitted - crit_val * .se.pred,
      upResp = .fitted + crit_val * .se.pred)

# Plot the intervals using geom_ribbon()
ggplot(predResp, aes(x=Biological, y=Foster)) + 
  geom_point() +
  stat_smooth(method="lm", se=FALSE) + 
  geom_ribbon(aes(ymin = lowResp, ymax = upResp), alpha = .2) +
  geom_ribbon(data = predMeans, aes(ymin = lowMean, ymax = upMean), alpha = .2, fill = "red")
```

# Technical conditions for linear regression

Linear model
Independent observations
Normally distributed around the line
Equal Variability around the line for all values of X

Linear model: residuals
- .fitted and .resid
- if linear model is appropriate, plot of residuals and fitted should be non patterned and scattered

## Exercises

Using residuals (1)
In the next few exercises, you will calculate residuals from a data set that complies with the linear regression technical conditions. For the linear model conditions to hold, the points should be scattered throughout the residual plot with no discernible pattern. Here, the residual plot looks like a scattering of points.

```{r}
# Plot the data
ggplot(hypdata_nice, aes(x=explanatory, y=response)) + 
  geom_point()

# Create and augmented model
nice_lm <- augment(lm(response~explanatory))

# Print the head of nice_lm
head(nice_lm)

# Plot the residuals
ggplot(nice_lm, aes(x=.fitted,  y=.resid)) + 
  geom_point()
```

Using residuals (2)
Now, you will calculate residuals from a data set that violates the technical conditions. For the linear model conditions to hold, the points should be scattered throughout the residual plot with no discernible pattern. Here the residuals reveal the violation of the technical conditions.

```{r}
# Plot the data
ggplot(hypdata_poor, aes(x=explanatory, y=response)) + 
  geom_point()

# Create an augmented model
poor_lm <- augment(lm(response~explanatory, data=hypdata_poor))

# Plot the residuals
ggplot(poor_lm, aes(x=.fitted, y=.resid)) + 
  geom_point()
```

# Effect of an outlier

Regression line changes based on presence of outliers
- better if you wanted to only model a subset of the observations

## Exercise

Estimation with and without outlier
The data provided in this exercise (hypdata_out) has an extreme outlier. You will run the linear model with and without the outlying point to see how one observation can affect the estimate of the line.

```{r}
# Plot the data and a linear model
ggplot(hypdata_out, aes(x=explanatory, y=response)) + 
  geom_point() +
  stat_smooth(method="lm", se=FALSE)

# Remove the outlier
hypdata_noout <- hypdata_out %>%
  filter(explanatory < 4.6)

# Plot all the data and both models
ggplot(hypdata_out, aes(x=explanatory, y=response)) + 
  geom_point() +
  stat_smooth(method="lm", se=FALSE) +
  stat_smooth(data=hypdata_noout, method="lm", color="red", se=FALSE)
```

Inference with and without outlier (t-test)
Not only can one point change the estimated regression line, but it can also change the inferential analysis. As before, in this exercise, you will run the analysis twice: once with the outlying value and once without it.

```{r}
# Examine the tidy model
tidy(lm(response~explanatory, data=hypdata_out))

# Remove the outlier
hypdata_noout <- hypdata_out %>%
  filter(explanatory < 4.6)

# Examine the new tidy model
tidy(lm(response~explanatory, data=hypdata_noout))
```

Inference with and without outlier (randomization)
Using the randomization test, you can again evaluate the effect of an outlier on the inferential conclusions of a linear model. Run a randomization test on the hypdata_out data twice: once with the outlying value and once without it. Note that the extended lines of code communicate clearly the steps of the randomization tests.

```{r}
# Calculate the p-value with the outlier
perm_slope_out %>% 
  mutate(abs_perm_slope = abs(stat)) %>%
  summarize(p_value = mean(abs_perm_slope > abs(obs_slope_out)))

# Calculate the p-value without the outlier
perm_slope_noout %>% 
  mutate(abs_perm_slope = abs(stat)) %>%
  summarize(p_value = mean(abs_perm_slope > abs(obs_slope_noout)))
```

# Moving forward when model assumptions are violated

do not remove outliers on a whim - use all of the data unless you're not interested in that subset

square the explanatory variable

transforming the response variable
- errors will be transformed too

## Exercises

Adjusting for non-linear relationship
The next three examples work with datasets where the underlying data structure violates the linear regression technical conditions. For each example, you will apply a transformation to the data in order to create residual plots that look scattered.

In this first example, it appears as though the variables are not linearly related.

```{r}
# Create an augmented model using the non-linear data
lm_nonlin <- augment(lm(response ~ explanatory, data_nonlin))

# Plot the residuals
ggplot(lm_nonlin, aes(x=.fitted, y=.resid)) +
  geom_point() + 
  geom_abline(slope = 0, intercept = 0)
  
# Create a second augmented model
lm2_nonlin <- augment(lm(response ~ explanatory + I(explanatory^2), data_nonlin))

# Plot the second set of residuals
ggplot(lm2_nonlin, aes(x=.fitted, y=.resid)) +
  geom_point() + 
  geom_abline(slope = 0, intercept = 0)
```

Keep in mind that squaring the input variable is great for modeling data that are better fit by a curved line (and really, when y is a function of x and x2).

Adjusting for non-constant errors
In this next example, it appears as though the variance of the response variable increases as the explanatory variable increases. Note that the fix in this exercise has the effect of changing both the variability as well as modifying the linearity of the relationship.

```{r}
# Create an augmented model
lm_nonequalvar <- augment(lm(response~explanatory, data=data_nonequalvar))

# Plot the residuals
ggplot(lm_nonequalvar, aes(x=.fitted, y=.resid)) + 
  geom_point() + 
  geom_abline(slope = 0, intercept = 0)

# Create an augmented model using the log of the response
lm2_nonequalvar <- augment(lm(log(response) ~ explanatory, data=data_nonequalvar))

# Plot the log of the resoponse
ggplot(data_nonequalvar, aes(x=explanatory, y=log(response))) + 
  stat_smooth(method="lm", se=FALSE) + 
  geom_point() +
  geom_abline(slope = 0, intercept = 0)
  
# Plot the second set of residuals
ggplot(lm2_nonequalvar, aes(x=.fitted, y=.resid)) + 
  geom_point() + 
  geom_abline(slope = 0, intercept = 0)
```

Keep in mind that using the log transformation on the response is good for data where the variance is unequal (and really, when ln(y) is a function of x).

Adjusting for non-normal errors
In this last example, it appears as though the points are not normally distributed around the regression line. Again, note that the fix in this exercise has the effect of changing both the variability as well as modifying the linearity of the relationship.

```{r}
# Create an augmented model of the data
lm_nonnorm <- augment(lm(response ~ explanatory, data=data_nonnorm))

# Plot the residuals
ggplot(lm_nonnorm, aes(x=.fitted, y=.resid)) + 
  geom_point() + 
  geom_abline(slope = 0, intercept = 0)

# Create the second augmented model
lm2_nonnorm <- augment(lm(sqrt(response) ~ explanatory, data=data_nonnorm))

# Plot the square root of the response
ggplot(data_nonnorm, aes(x=explanatory, y=sqrt(response))) + 
  geom_point() + 
  stat_smooth(method="lm", se=FALSE)
  
# Plot the second set of residuals
ggplot(lm2_nonnorm, aes(x=.fitted, y=.resid)) + 
  geom_point() + 
  geom_abline(slope = 0, intercept = 0)
```

Keep in mind that using the square root transformation on the response is good for data where the data are not normal around the line (and really, when sqrt(y) is a function of x).

# Inference on transformed variables

## Exercises

Transformed model
As you saw in the previous chapter, transforming the variables can often transform a model from one where the technical conditions are violated to one where the technical conditions hold. When technical conditions hold, you are able to accurately interpret the inferential output. In the two models below, note how the standard errors and p-values change (although in both settings the p-value is significant).

```{r}
# Create a tidy model
tidy(lm(price ~ sqft, data=LAhomes))

# Create a tidy model using the log of both variables
tidy(lm(log(price) ~ log(sqft), data=LAhomes))
```

# Multicollinearity

Amount vs coins is linearly related
Amount vs small coins is linearly related
Multiple variables then slope is negative

## Exercises

LA Homes, multicollinearity (1)
In the next series of exercises, you will investigate how to interpret the sign (positive or negative) of the slope coefficient as well as the significance of the variables (p-value). You will continue to use the log transformed variables so that the technical conditions hold, but you will not be concerned here with the value of the coefficient.

```{r}
# Output the tidy model
tidy(lm(log(price) ~ log(sqft), data=LAhomes))
```

LA Homes, multicollinearity (2)
Repeat the previous exercise, but this time regress the log transformed variable price on the new variable bath which records the number of bathrooms in a home.

```{r}
# Output the tidy model
tidy(lm(log(price)~log(bath), data=LAhomes))
```

LA Homes, multicollinearity (3)
Now, regress the log transformed variable price on the log transformed variables sqft AND bath. The model is a three dimensional linear regression model where you are predicting price as a plane (think of a piece of paper) above the axes including both sqft and bath.

```{r}
# Output the tidy model
tidy(lm(log(price)~log(sqft) + log(bath), data=LAhomes))
```

# Multiple linear regression

using multiple variables for linear regression
p.value tells you if the variables are significant (close to 0)

## Exercises

Inference on coefficients
Using the NYC Italian restaurants dataset (compiled by Simon Sheather in A Modern Approach to Regression with R), restNYC, you will investigate the effect on the significance of the coefficients when there are multiple variables in the model. Recall, the p-value associated with any coefficient is the probability of the observed data given that the particular variable is independent of the response AND given that all other variables are included in the model.

```{r}
# Output the first model
tidy(lm(Price ~ Service, data=restNYC))

# Output the second model
tidy(lm(Price ~ Service + Food + Decor, data=restNYC))
```