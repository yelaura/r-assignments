---
title: "Supervised Learning: Classification - Datacamp"
output: html_notebook
author: "Laura Ye"
---

# Classification with nearest neighbors

Subset of machine learning: supervised learning
Learning from prior examples
Classification: Identifying from a set of categories
Tasks are diverse and common

Use machine learning to classify sign types

Images => actions
Stop signs => stop

Signs that look alike should be the same sign

Measuring similarity with distance
- feature space - e.g. color
- euclidean distance formula

K-nearest neighbors (Knn)
- looks for the observation closest to the observed one

```{r}
library(class)
pred <- knn(training_data, testing_data, training_labels)
```

## Exercises

Recognizing a road sign with kNN
After several trips with a human behind the wheel, it is time for the self-driving car to attempt the test course alone.

As it begins to drive away, its camera captures the following image:

Can you apply a kNN classifier to help the car recognize this sign?

```{r}
# Load the 'class' package
library(class)

# Create a vector of labels
sign_types <- signs$sign_type

# Classify the next sign observed
knn(train = signs[-1], test = next_sign, cl = sign_types)
```

Exploring the traffic sign dataset
To better understand how the knn() function was able to classify the stop sign, it may help to examine the training dataset it used.

Each previously observed street sign was divided into a 4x4 grid, and the red, green, and blue level for each of the 16 center pixels is recorded as illustrated here.

The result is a dataset that records the sign_type as well as 16 x 3 = 48 color properties of each sign.

```{r}
# Examine the structure of the signs dataset
str(signs)

# Count the number of signs of each type
table(signs$sign_type)

# Check r10's average red level by sign type
aggregate(r10 ~ sign_type, data = signs, mean)
```

Classifying a collection of road signs
Now that the autonomous vehicle has successfully stopped on its own, your team feels confident allowing the car to continue the test course.

The test course includes 59 additional road signs divided into three types:

At the conclusion of the trial, you are asked to measure the car's overall performance at recognizing these signs.

```{r}
# Use kNN to identify the test road signs
sign_types <- signs$sign_type
signs_pred <- knn(train = signs[-1], test = test_signs[-1], cl = sign_types)

# Create a confusion matrix of the actual versus predicted values
signs_actual <- test_signs$sign_type
table(signs_actual, signs_pred)

# Compute the accuracy
mean(signs_actual == signs_pred)
```

# What about the k in kNN?

What is k?
Letter k is a variable that specifies how many neighbors to consider.
Ignored k and default has been used (1)
Single nearest most similar neighbor was used.

If K>1, the category with the most votes would be the prediction.
In a tie, winner is decided at random.
Not always better to have large k

Setting k larger ignores some potentially noisy points
Smaller k has fuzzy boundaries

Rule of thumb k=sqrt(nrow)

## Exercises

Testing other 'k' values
By default, the knn() function in the class package uses only the single nearest neighbor.

Setting a k parameter allows the algorithm to consider additional nearby neighbors. This enlarges the collection of neighbors which will vote on the predicted class.

Compare k values of 1, 7, and 15 to examine the impact on traffic sign classification accuracy.

```{r}
# Compute the accuracy of the baseline model (default k = 1)
k_1 <- knn(train = signs[-1], test = signs_test[-1], cl = signs$sign_type)
mean(k_1 == signs_actual)

# Modify the above to set k = 7
k_7 <- knn(train = signs[-1], test = signs_test[-1], cl = signs$sign_type, k=7)
mean(k_7 == signs_actual)

# Set k = 15 and compare to the above
k_15 <- knn(train = signs[-1], test = signs_test[-1], cl = signs$sign_type, k=15)
mean(k_15 == signs_actual)
```

Seeing how the neighbors voted
When multiple nearest neighbors hold a vote, it can sometimes be useful to examine whether the voters were unanimous or widely separated.

For example, knowing more about the voters' confidence in the classification could allow an autonomous vehicle to use caution in the case there is any chance at all that a stop sign is ahead.

In this exercise, you will learn how to obtain the voting results from the knn() function.

```{r}
# Use the prob parameter to get the proportion of votes for the winning class
sign_pred <- knn(train = signs[-1], test = signs_test[-1], cl = sign_types, k = 7, prob = TRUE)

# Get the "prob" attribute from the predicted classes
sign_prob <- attr(sign_pred, "prob")

# Examine the first several predictions
head(sign_pred)

# Examine the proportion of votes for the winning class
head(sign_prob)
```

# Data Preparation for kNN

kNN assumes numeric data - distance formula
- hard to find distance between categories
- use dummy coding to denote shapes, for instance

kNN benefits from normalized data or same range of values
- features with wider range will affect distance more than the features with smaller ranges
- R does not have a built-in function to normalize

# Understanding Bayesian methods

Phone can forecast probable future location based on past locations
Bayesian methods - rules for estimating probabilities in light of historical data

Probability = # of times the event happened / # of observations
Can combine information from several events (time of day) to form a prediction using a venn diagram
E.g. greater probability of being at work in the afternoon than evening

P(A and B)
Independent events - unrelated events
E.g. other users' locations are independent events
Dependent events - one event depends on the other
P(A | B) = P(A and B) / P(B)

Naive Bayes applies Bayesian probabilities

```{r}
library(naivebayes)
naive_bayes(location ~ time_of_day, data = location_history)
```

## Exercises

Computing probabilities
The where9am data frame contains 91 days (thirteen weeks) worth of data in which Brett recorded his location at 9am each day as well as whether the daytype was a weekend or weekday.

Using the conditional probability formula below, you can compute the probability that Brett is working in the office, given that it is a weekday.

P(A|B)=P(A and B)P(B)
Calculations like these are the basis of the Naive Bayes destination prediction model you'll develop in later exercises.

```{r}
# Compute P(A) 
p_A <- nrow(subset(where9am, location=="office")) / nrow(where9am)

# Compute P(B)
p_B <- nrow(subset(where9am, daytype=="weekday")) / nrow(where9am)

# Compute the observed P(A and B)
p_AB <- nrow(subset(where9am, daytype=="weekday" & location == "office")) / nrow(where9am)

# Compute P(A | B)
p_A_given_B <- p_AB / p_B
print(p_A_given_B)
```

A simple Naive Bayes location model
The previous exercises showed that the probability that Brett is at work or at home at 9am is highly dependent on whether it is the weekend or a weekday.

To see this finding in action, use the where9am data frame to build a Naive Bayes model on the same data.

You can then use this model to predict the future: where does the model think that Brett will be at 9am on Thursday and at 9am on Saturday?

```{r}
# Load the naivebayes package
library(naivebayes)

# Build the location prediction model
locmodel <- naive_bayes(location ~ daytype, data = where9am)

# Predict Thursday's 9am location
predict(locmodel, thursday9am)

# Predict Saturdays's 9am location
predict(locmodel, saturday9am)
```

Examining "raw" probabilities
The naivebayes package offers several ways to peek inside a Naive Bayes model.

Typing the name of the model object provides the a priori (overall) and conditional probabilities of each of the model's predictors. If one were so inclined, you might use these for calculating posterior (predicted) probabilities by hand.

Alternatively, R will compute the posterior probabilities for you if the type = "prob" parameter is supplied to the predict() function.

Using these methods, examine how the model's predicted 9am location probability varies from day-to-day.

```{r}
# The 'naivebayes' package is loaded into the workspace
# and the Naive Bayes 'locmodel' has been built

# Examine the location prediction model
print(locmodel)

# Obtain the predicted probabilities for Thursday at 9am
predict(locmodel, thursday9am , type = "prob")

# Obtain the predicted probabilities for Saturday at 9am
predict(locmodel, saturday9am , type = "prob")
```

# Understanding NB's naivety

Adding more features complicates the model

Challenge of multiple predictors
- venn diagram looks confusing and messy
- inefficient to calculate overlap
- uses shortcut to approximate the probability

Naive simplification
- assumes events are independent
- joint events are computed by multiplying the simpler events
- performs well even though this assumption does not apply to all real-world problems

Infrequent problem
- events chained together
- event that has never been observed
- venn diagram has no overlap and probability = 0
- when 0 is multiplied in a chain => the whole probability is 0
- Solution: Laplace correction - add 1 to each event to eliminate the veto power of 0

## Exercises

A more sophisticated location model
The locations dataset records Brett's location every hour for 13 weeks. Each hour, the tracking information includes the daytype (weekend or weekday) as well as the hourtype (morning, afternoon, evening, or night).

Using this data, build a more sophisticated model to see how Brett's predicted location not only varies by the day of week but also by the time of day.

```{r}
# The 'naivebayes' package is loaded into the workspace already

# Build a NB model of location
locmodel <- naive_bayes(location ~ daytype + hourtype, locations)

# Predict Brett's location on a weekday afternoon
predict(locmodel, weekday_afternoon)

# Predict Brett's location on a weekday evening
predict(locmodel, weekday_evening)
```

Preparing for unforeseen circumstances
While Brett was tracking his location over 13 weeks, he never went into the office during the weekend. Consequently, the joint probability of P(office and weekend) = 0.

Explore how this impacts the predicted probability that Brett may go to work on the weekend in the future. Additionally, you can see how using the Laplace correction will allow a small chance for these types of unforeseen circumstances.

```{r}
# The 'naivebayes' package is loaded into the workspace already
# The Naive Bayes location model (locmodel) has already been built

# Observe the predicted probabilities for a weekend afternoon
predict(locmodel, weekend_afternoon, type="prob")

# Build a new model using the Laplace correction
locmodel2 <- naive_bayes(location ~ daytype + hourtype, locations, laplace=1)

# Observe the new predicted probabilities for a weekend afternoon
predict(locmodel2, weekend_afternoon, type="prob")
```

# Applying Naive Bayes to other problems

Works well with information from multiple attributes that need to be considered simulataneously and analyzed as a whole
Frequently used for classifying text data (spam)

Challenges when applying NB

How NB uses data
- Computes conditional probability of events and outcomes
- Probabilities are multiplied
- Numeric data and structured text data is hard to process
- binning - create categories from numeric data (e.g. percentiles)
-- use more meaningful bins like temperature is cold, warm, hot

Preparing text for NB
- bag of words
- does not consider word order, grammar, and semantics
- creates an event for each collection of text documents
- Wide table - rows are documents and columns are words that appear in doc
-- each cell says yes or no if the word appears in the document

## Exercises

